{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 2950: Phase 2 \n",
    "#### Group Members: Anusha Bishayee, Katheryn Ding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### __Research Question:__  \n",
    "\n",
    "#### How do ESG score and stock performance (price) align across different industries? What associations can we find between company industry, stock performance, and ESG ratings?\n",
    "#### note: ESG score refers to a quantiative metric measuring a company's environmental, social, and governance performance; 'environmental' pertains to aspects like waste management and energy emissions, 'social' pertains to aspects like customer satisfaction and DEI in the workplace, and 'governance' pertains to aspects like operating efficiencies and risk management. ESG scores are typically examined by independent investors, business analysts, and even competitior companies to assess risk or opportunities associated with a specific company's practices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### __Data Collection and Cleaning:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sys\n",
    "import warnings\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our original dataset with ESG information for different large/mid-cap companies came in a csv format, which we downloaded from Kaggle. this had about 722 rows, each corresponding to a unique publicly traded company. further description of the columns here can be found in the 'Dataset Description' portion of this notebook.\n",
    "\n",
    "we first dropped all rows that had null values, which eliminated 27 companies. we then filtered this dataset for just USD currency, excluding companies that are traded in CNY or any other currency. this allows us to have greater familiarity with the industries and companies we analyze - this process eliminated 15 more of our rows, and left us with 680 companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data shape: (722, 21)\n",
      "non-null data shape: (695, 21)\n",
      "refined data shape: (680, 21)\n"
     ]
    }
   ],
   "source": [
    "esg = pd.read_csv(\"esg_data.csv\")\n",
    "print(f\"original data shape: {esg.shape}\")\n",
    "\n",
    "esg = esg.dropna()\n",
    "print(f\"non-null data shape: {esg.shape}\")\n",
    "\n",
    "esg = esg[esg[\"currency\"] == \"USD\"]\n",
    "print(f\"refined data shape: {esg.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then, we converted the 'last_processing_date' column in our dataset to DateTime format - a lot of rows had a differing date formats as well, so we had to convert them all to m/d/y. after that, we sorted the dataset by ascending and descending 'last_processing_date' to see the range of processing dates in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest dates:\n",
      "720    11-15-2022\n",
      "716    11-15-2022\n",
      "Name: last_processing_date, dtype: object\n",
      "\n",
      "earliest dates:\n",
      "658    02-08-2022\n",
      "36     04-16-2022\n",
      "Name: last_processing_date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "esg[\"last_processing_date\"] = pd.to_datetime(esg[\"last_processing_date\"], format = \"mixed\")\n",
    "esg[\"last_processing_date\"] = esg[\"last_processing_date\"].dt.strftime('%m-%d-%Y')\n",
    "\n",
    "esg = esg.sort_values(by = \"last_processing_date\", ascending = False)\n",
    "print(f\"latest dates:\\n{esg[\"last_processing_date\"].head(2)}\")\n",
    "\n",
    "esg = esg.sort_values(by = \"last_processing_date\", ascending = True)\n",
    "print(f\"\\nearliest dates:\\n{esg[\"last_processing_date\"].head(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, we wanted to add our finance data from the yfinance library onto to our esg dataset. we used the ticker column to match up companies from the yfinance library and our esg dataset, and we set our dates of the finance data to range from 2/1/21 to 12/31/22, as all of the 'last processing date' values for the esg data range from 2/8/22 to 11/15/22. in specific, we calculated a stock percentage change over this period for each company, a volatility index, a 50-day moving average, and a cumulative return metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevents some annoying yfinance outputs from printing\n",
    "@contextlib.contextmanager\n",
    "def suppress_output():\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        old_stderr = sys.stderr\n",
    "        sys.stdout = devnull\n",
    "        sys.stderr = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "            sys.stderr = old_stderr\n",
    "\n",
    "tickers = esg[\"ticker\"].tolist()\n",
    "stock_data = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        # suppresses all of the outputs when grabbing data from yfinance\n",
    "        with suppress_output():  \n",
    "            stock = yf.download(ticker, start = \"2021-02-01\", end = \"2022-12-31\", progress = False)\n",
    "        \n",
    "        if not stock.empty:\n",
    "            # get closing price for 01/01/2021 and 12/31/2022\n",
    "            close_2021_02_01 = stock.loc[\"2021-02-01\"][\"Close\"] if \"2021-02-01\" in stock.index else None\n",
    "            close_2022_12_31 = stock.loc[\"2022-12-30\"][\"Close\"] if \"2022-12-30\" in stock.index else None\n",
    "\n",
    "            # calculating percentage change\n",
    "            percentage_change = ((stock[\"Close\"].iloc[-1] - stock[\"Close\"].iloc[0]) / stock[\"Close\"].iloc[0]) * 100\n",
    "            \n",
    "            # calculating volatility (sd of daily returns)\n",
    "            daily_returns = stock[\"Close\"].pct_change()\n",
    "            volatility = daily_returns.std()\n",
    "            \n",
    "            # calculating 50-day moving average\n",
    "            stock[\"50_day_SMA\"] = stock[\"Close\"].rolling(window=50).mean()\n",
    "            sma_50_day = stock[\"50_day_SMA\"].iloc[-1]\n",
    "            \n",
    "            # calculating cumulative return\n",
    "            cumulative_return = (stock[\"Close\"].iloc[-1] / stock[\"Close\"].iloc[0]) - 1\n",
    "            \n",
    "            stock_data.append({\n",
    "                'ticker': ticker, \n",
    "                'start_close': close_2021_02_01,\n",
    "                'end_close': close_2022_12_31,\n",
    "                'percentage_change': percentage_change,\n",
    "                'volatility': volatility,\n",
    "                '50_day_SMA': sma_50_day,\n",
    "                'cumulative_return': cumulative_return\n",
    "            })\n",
    "            \n",
    "    # also helps to suppress annoying outputs\n",
    "    except (yf.YFTzMissingError, yf.YFPricesMissingError):\n",
    "        pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, we needed to convert the stock data we extracted from yfinance to a dataframe, so that we can merge it with our original esg dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current data shape: (680, 27)\n",
      "\n",
      "  ticker                           name currency  \\\n",
      "0   poww                       Ammo Inc      USD   \n",
      "1   acls       Axcelis Technologies Inc      USD   \n",
      "2   achc  Acadia Healthcare Company Inc      USD   \n",
      "3     cf     CF Industries Holdings Inc      USD   \n",
      "4      t                       AT&T Inc      USD   \n",
      "\n",
      "                        exchange           industry  \\\n",
      "0     NASDAQ NMS - GLOBAL MARKET   Leisure Products   \n",
      "1     NASDAQ NMS - GLOBAL MARKET     Semiconductors   \n",
      "2     NASDAQ NMS - GLOBAL MARKET        Health Care   \n",
      "3  NEW YORK STOCK EXCHANGE, INC.          Chemicals   \n",
      "4  NEW YORK STOCK EXCHANGE, INC.  Telecommunication   \n",
      "\n",
      "                                                logo  \\\n",
      "0  https://static.finnhub.io/logo/8decc6ca0564a89...   \n",
      "1  https://static.finnhub.io/logo/88b5f730-80df-1...   \n",
      "2  https://static.finnhub.io/logo/4b6b2e5a4cfce5b...   \n",
      "3  https://static.finnhub.io/logo/9b57a636-80eb-1...   \n",
      "4  https://static.finnhub.io/logo/7d20269e-80ec-1...   \n",
      "\n",
      "                              weburl environment_grade environment_level  \\\n",
      "0               https://ammoinc.com/                 B            Medium   \n",
      "1           https://www.axcelis.com/                 A              High   \n",
      "2  https://www.acadiahealthcare.com/                BB            Medium   \n",
      "3      https://www.cfindustries.com/                 A              High   \n",
      "4               https://www.att.com/                 B            Medium   \n",
      "\n",
      "  social_grade  ... last_processing_date total_grade total_level      cik  \\\n",
      "0            B  ...           02-08-2022           B      Medium  1015383   \n",
      "1           BB  ...           04-16-2022         BBB        High  1113232   \n",
      "2            B  ...           04-16-2022          BB      Medium  1520697   \n",
      "3           BB  ...           04-16-2022         BBB        High  1324404   \n",
      "4            B  ...           04-16-2022           B      Medium   732717   \n",
      "\n",
      "   start_close  end_close  percentage_change volatility 50_day_SMA  \\\n",
      "0     5.400000   1.730000         -67.962963   0.040577     2.3782   \n",
      "1    36.150002  79.360001         119.529730   0.038259    73.6454   \n",
      "2    52.990002  82.320000          55.350061   0.021303    82.6706   \n",
      "3    42.970001  85.199997          98.277856   0.027433   101.2440   \n",
      "4    21.638973  18.410000         -14.922027   0.015132    18.5720   \n",
      "\n",
      "  cumulative_return  \n",
      "0         -0.679630  \n",
      "1          1.195297  \n",
      "2          0.553501  \n",
      "3          0.982779  \n",
      "4         -0.149220  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "stock_df = pd.DataFrame(stock_data)\n",
    "merged_df = esg.merge(stock_df, on = 'ticker', how = 'left')\n",
    "print(f\"current data shape: {merged_df.shape}\")\n",
    "print(f\"\\n{merged_df.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after that, we needed to drop any rows where the finance data join has left null values. this eliminates 80 more of our rows, leaving us with 600 companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-null finance data shape: (600, 27)\n",
      "\n",
      "<bound method NDFrame.head of     ticker                             name currency  \\\n",
      "71     mmm                            3M Co      USD   \n",
      "142    aos                   A O Smith Corp      USD   \n",
      "661   abvc               ABVC Biopharma Inc      USD   \n",
      "17    acad       ACADIA Pharmaceuticals Inc      USD   \n",
      "41    aciw                ACI Worldwide Inc      USD   \n",
      "..     ...                              ...      ...   \n",
      "500    zts                       Zoetis Inc      USD   \n",
      "571    zuo                        Zuora Inc      USD   \n",
      "643    zws  Zurn Elkay Water Solutions Corp      USD   \n",
      "647   zyme                    Zymeworks Inc      USD   \n",
      "189   ebay                         eBay Inc      USD   \n",
      "\n",
      "                          exchange                  industry  \\\n",
      "71   NEW YORK STOCK EXCHANGE, INC.  Industrial Conglomerates   \n",
      "142  NEW YORK STOCK EXCHANGE, INC.                  Building   \n",
      "661     NASDAQ NMS - GLOBAL MARKET             Biotechnology   \n",
      "17      NASDAQ NMS - GLOBAL MARKET             Biotechnology   \n",
      "41      NASDAQ NMS - GLOBAL MARKET                Technology   \n",
      "..                             ...                       ...   \n",
      "500  NEW YORK STOCK EXCHANGE, INC.           Pharmaceuticals   \n",
      "571  NEW YORK STOCK EXCHANGE, INC.                Technology   \n",
      "643  NEW YORK STOCK EXCHANGE, INC.                  Building   \n",
      "647  NEW YORK STOCK EXCHANGE, INC.             Biotechnology   \n",
      "189     NASDAQ NMS - GLOBAL MARKET                    Retail   \n",
      "\n",
      "                                                  logo  \\\n",
      "71   https://static.finnhub.io/logo/2a1802fa-80ec-1...   \n",
      "142  https://static.finnhub.io/logo/73381be8-80eb-1...   \n",
      "661  https://static.finnhub.io/logo/2b1c9fbcfafa70d...   \n",
      "17   https://static.finnhub.io/logo/2d87da57d47f7a5...   \n",
      "41   https://static.finnhub.io/logo/875c5a76-80df-1...   \n",
      "..                                                 ...   \n",
      "500  https://static.finnhub.io/logo/aae505a6-80cd-1...   \n",
      "571  https://static.finnhub.io/logo/181c3c26-80db-1...   \n",
      "643  https://static.finnhub.io/logo/166822dc-80c9-1...   \n",
      "647  https://static.finnhub.io/logo/d5054e357d522c9...   \n",
      "189  https://static.finnhub.io/logo/919a6270-826a-1...   \n",
      "\n",
      "                              weburl environment_grade environment_level  \\\n",
      "71               https://www.3m.com/                 A              High   \n",
      "142         https://www.aosmith.com/                 A              High   \n",
      "661          https://abvcpharma.com/                 B            Medium   \n",
      "17               https://acadia.com/                 B            Medium   \n",
      "41     https://www.aciworldwide.com/                 A              High   \n",
      "..                               ...               ...               ...   \n",
      "500          https://www.zoetis.com/                 A              High   \n",
      "571           https://www.zuora.com/                 B            Medium   \n",
      "643  https://zurnwatersolutions.com/                 A              High   \n",
      "647       https://www.zymeworks.com/                 B            Medium   \n",
      "189         https://www.ebayinc.com/                 A              High   \n",
      "\n",
      "    social_grade  ... last_processing_date total_grade total_level      cik  \\\n",
      "71            BB  ...           04-16-2022         BBB        High    66740   \n",
      "142           BB  ...           04-16-2022         BBB        High    91142   \n",
      "661            B  ...           11-06-2022           B      Medium  1173313   \n",
      "17             B  ...           04-16-2022          BB      Medium  1070494   \n",
      "41            BB  ...           04-16-2022         BBB        High   935036   \n",
      "..           ...  ...                  ...         ...         ...      ...   \n",
      "500           BB  ...           04-20-2022         BBB        High  1555280   \n",
      "571            B  ...           09-24-2022           B      Medium  1423774   \n",
      "643           BB  ...           11-06-2022         BBB        High  1439288   \n",
      "647           BB  ...           11-06-2022          BB      Medium  1403752   \n",
      "189           BB  ...           04-17-2022         BBB        High  1065088   \n",
      "\n",
      "     start_close   end_close  percentage_change volatility  50_day_SMA  \\\n",
      "71    146.070236  100.267555         -31.356615   0.014472  104.066388   \n",
      "142    56.029999   57.240002           2.159563   0.018196   57.343600   \n",
      "661    50.000000    6.250000         -87.500000   0.089921    7.173200   \n",
      "17     46.580002   15.920000         -65.822242   0.045374   15.391400   \n",
      "41     39.590000   23.000000         -41.904522   0.021739   21.438400   \n",
      "..           ...         ...                ...        ...         ...   \n",
      "500   155.580002  146.550003          -5.804087   0.016947  147.633001   \n",
      "571    14.240000    6.360000         -55.337077   0.034177    7.057200   \n",
      "643    38.889999   21.150000         -45.615840   0.032776   23.315800   \n",
      "647    35.639999    7.860000         -77.946127   0.049518    7.356000   \n",
      "189    58.470001   41.470001         -29.074739   0.022532   42.343200   \n",
      "\n",
      "    cumulative_return  \n",
      "71          -0.313566  \n",
      "142          0.021596  \n",
      "661         -0.875000  \n",
      "17          -0.658222  \n",
      "41          -0.419045  \n",
      "..                ...  \n",
      "500         -0.058041  \n",
      "571         -0.553371  \n",
      "643         -0.456158  \n",
      "647         -0.779461  \n",
      "189         -0.290747  \n",
      "\n",
      "[600 rows x 27 columns]>\n"
     ]
    }
   ],
   "source": [
    "merged_df = merged_df.dropna()\n",
    "print(f\"non-null finance data shape: {merged_df.shape}\\n\")\n",
    "\n",
    "merged_df = merged_df.sort_values(by = \"name\", ascending = True)\n",
    "print(merged_df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is still a lot of data, but will be helpful for getting industry-level and other general overviews of the data. merged_df will be our main dataset.\n",
    "\n",
    "we also want to create a sample of these 600 companies so that we are able to look at trends and associations at individual company-level as well. to create our sample, we went through the industry list and selected 23 well-known companies from differing industries manually, however, we will probably refine this approach later (this ties into one of our questions for reviewers and limitations). sample_companies will be our 2nd dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ticker                                 name currency  \\\n",
      "161   abbv                           Abbvie Inc      USD   \n",
      "147    are  Alexandria Real Estate Equities Inc      USD   \n",
      "6      aal          American Airlines Group Inc      USD   \n",
      "10    aapl                            Apple Inc      USD   \n",
      "133    bac                 Bank of America Corp      USD   \n",
      "660   bbwi                Bath & Body Works Inc      USD   \n",
      "130    bdx              Becton Dickinson and Co      USD   \n",
      "121    bro                    Brown & Brown Inc      USD   \n",
      "86     cmg           Chipotle Mexican Grill Inc      USD   \n",
      "273    duk                     Duke Energy Corp      USD   \n",
      "235     gm                    General Motors Co      USD   \n",
      "231     gs              Goldman Sachs Group Inc      USD   \n",
      "508    mro                    Marathon Oil Corp      USD   \n",
      "512   meta                   Meta Platforms Inc      USD   \n",
      "362   pypl                  PayPal Holdings Inc      USD   \n",
      "452    crm                       Salesforce Inc      USD   \n",
      "450   sbux                       Starbucks Corp      USD   \n",
      "429   tmus                      T-Mobile US Inc      USD   \n",
      "471    tgt                          Target Corp      USD   \n",
      "430   tsla                            Tesla Inc      USD   \n",
      "428    ual         United Airlines Holdings Inc      USD   \n",
      "465    dis                       Walt Disney Co      USD   \n",
      "189   ebay                             eBay Inc      USD   \n",
      "\n",
      "                          exchange                        industry  \\\n",
      "161  NEW YORK STOCK EXCHANGE, INC.                   Biotechnology   \n",
      "147  NEW YORK STOCK EXCHANGE, INC.                     Real Estate   \n",
      "6       NASDAQ NMS - GLOBAL MARKET                        Airlines   \n",
      "10      NASDAQ NMS - GLOBAL MARKET                      Technology   \n",
      "133  NEW YORK STOCK EXCHANGE, INC.                         Banking   \n",
      "660  NEW YORK STOCK EXCHANGE, INC.                          Retail   \n",
      "130  NEW YORK STOCK EXCHANGE, INC.                     Health Care   \n",
      "121  NEW YORK STOCK EXCHANGE, INC.                       Insurance   \n",
      "86   NEW YORK STOCK EXCHANGE, INC.  Hotels Restaurants and Leisure   \n",
      "273  NEW YORK STOCK EXCHANGE, INC.                       Utilities   \n",
      "235  NEW YORK STOCK EXCHANGE, INC.                     Automobiles   \n",
      "231  NEW YORK STOCK EXCHANGE, INC.              Financial Services   \n",
      "508  NEW YORK STOCK EXCHANGE, INC.                          Energy   \n",
      "512     NASDAQ NMS - GLOBAL MARKET                           Media   \n",
      "362     NASDAQ NMS - GLOBAL MARKET                      Technology   \n",
      "452  NEW YORK STOCK EXCHANGE, INC.                      Technology   \n",
      "450     NASDAQ NMS - GLOBAL MARKET  Hotels Restaurants and Leisure   \n",
      "429     NASDAQ NMS - GLOBAL MARKET               Telecommunication   \n",
      "471  NEW YORK STOCK EXCHANGE, INC.                          Retail   \n",
      "430     NASDAQ NMS - GLOBAL MARKET                     Automobiles   \n",
      "428     NASDAQ NMS - GLOBAL MARKET                        Airlines   \n",
      "465  NEW YORK STOCK EXCHANGE, INC.                           Media   \n",
      "189     NASDAQ NMS - GLOBAL MARKET                          Retail   \n",
      "\n",
      "                                                  logo  \\\n",
      "161  https://static.finnhub.io/logo/8806d72c-80cd-1...   \n",
      "147  https://static.finnhub.io/logo/ffde2ccf9a1b68d...   \n",
      "6    https://static2.finnhub.io/file/publicdatany/f...   \n",
      "10   https://static.finnhub.io/logo/87cb30d8-80df-1...   \n",
      "133  https://static2.finnhub.io/file/publicdatany/f...   \n",
      "660  https://static.finnhub.io/logo/1b846d4a-80ec-1...   \n",
      "130  https://static.finnhub.io/logo/82e1f20c-80eb-1...   \n",
      "121  https://static.finnhub.io/logo/8d4bb0d8-80eb-1...   \n",
      "86   https://static.finnhub.io/logo/a551a6c2-80eb-1...   \n",
      "273  https://static.finnhub.io/logo/c6386312-80eb-1...   \n",
      "235  https://static.finnhub.io/logo/9253db78-80c9-1...   \n",
      "231  https://static.finnhub.io/logo/7ba3bd9e8fc91f9...   \n",
      "508  https://static.finnhub.io/logo/2f57969a-80ec-1...   \n",
      "512  https://static2.finnhub.io/file/publicdatany/f...   \n",
      "362  https://static.finnhub.io/logo/58e310c0-80d2-1...   \n",
      "452  https://static.finnhub.io/logo/5a800a68f67c85e...   \n",
      "450  https://static.finnhub.io/logo/8d4c97ac-80e0-1...   \n",
      "429  https://static.finnhub.io/logo/4ffaa8b8-80ec-1...   \n",
      "471  https://static.finnhub.io/logo/83bbf858-80ec-1...   \n",
      "430  https://static.finnhub.io/logo/2dd96524-80c9-1...   \n",
      "428  https://static.finnhub.io/logo/b3f34b67dcba172...   \n",
      "465  https://static.finnhub.io/logo/ef50b4a2b263c84...   \n",
      "189  https://static.finnhub.io/logo/919a6270-826a-1...   \n",
      "\n",
      "                                    weburl environment_grade  \\\n",
      "161                https://www.abbvie.com/                 A   \n",
      "147                   https://www.are.com/                AA   \n",
      "6    https://americanairlines.gcs-web.com/                 B   \n",
      "10                  https://www.apple.com/                BB   \n",
      "133          https://www.bankofamerica.com                 B   \n",
      "660                https://www.bbwinc.com/                 A   \n",
      "130                    https://www.bd.com/                AA   \n",
      "121           https://www.bbinsurance.com/               BBB   \n",
      "86               https://www.chipotle.com/                 B   \n",
      "273           https://www.duke-energy.com/                AA   \n",
      "235                    https://www.gm.com/                 A   \n",
      "231          https://www.goldmansachs.com/                 A   \n",
      "508           https://www.marathonoil.com/                 A   \n",
      "512               https://www.facebook.com                 B   \n",
      "362                https://www.paypal.com/                BB   \n",
      "452            https://www.salesforce.com/                 B   \n",
      "450             https://www.starbucks.com/               BBB   \n",
      "429              https://www.t-mobile.com/                BB   \n",
      "471          https://corporate.target.com/                 A   \n",
      "430                 https://www.tesla.com/                 A   \n",
      "428                 https://ir.united.com/                 B   \n",
      "465      https://thewaltdisneycompany.com/                 A   \n",
      "189               https://www.ebayinc.com/                 A   \n",
      "\n",
      "    environment_level social_grade  ... last_processing_date total_grade  \\\n",
      "161              High           BB  ...           04-16-2022         BBB   \n",
      "147         Excellent           BB  ...           04-16-2022           A   \n",
      "6              Medium            B  ...           04-16-2022           B   \n",
      "10             Medium            B  ...           04-16-2022          BB   \n",
      "133            Medium           BB  ...           04-16-2022          BB   \n",
      "660              High           BB  ...           11-06-2022         BBB   \n",
      "130         Excellent           BB  ...           04-16-2022           A   \n",
      "121              High           BB  ...           04-16-2022         BBB   \n",
      "86             Medium            B  ...           04-16-2022           B   \n",
      "273         Excellent           BB  ...           04-17-2022           A   \n",
      "235              High           BB  ...           04-17-2022         BBB   \n",
      "231              High           BB  ...           04-17-2022         BBB   \n",
      "508              High           BB  ...           04-21-2022         BBB   \n",
      "512            Medium            B  ...           04-22-2022           B   \n",
      "362            Medium            B  ...           04-18-2022          BB   \n",
      "452            Medium            B  ...           04-19-2022           B   \n",
      "450              High           BB  ...           04-19-2022         BBB   \n",
      "429            Medium           BB  ...           04-19-2022         BBB   \n",
      "471              High           BB  ...           04-19-2022           A   \n",
      "430              High          CCC  ...           04-19-2022         BBB   \n",
      "428            Medium            B  ...           04-19-2022           B   \n",
      "465              High           BB  ...           04-19-2022         BBB   \n",
      "189              High           BB  ...           04-17-2022         BBB   \n",
      "\n",
      "    total_level      cik  start_close   end_close  percentage_change  \\\n",
      "161        High  1551152   102.300003  161.610001          57.976535   \n",
      "147        High  1035443   169.559998  145.669998         -14.089408   \n",
      "6        Medium     6201    16.840000   12.720000         -24.465557   \n",
      "10       Medium   320193   134.139999  129.929993          -3.138517   \n",
      "133      Medium    70858    29.959999   33.119999          10.547396   \n",
      "660        High   701985    33.839935   42.139999          24.527423   \n",
      "130        High    10795   254.995117  254.300003          -0.272599   \n",
      "121        High    79282    42.959999   56.970001          32.611738   \n",
      "86       Medium  1058090    30.180401   27.749800          -8.053575   \n",
      "273        High  1326160    93.160004  102.989998          10.551732   \n",
      "235        High  1467858    51.509998   33.639999         -34.692292   \n",
      "231        High   886982   274.730011  343.380005          24.988167   \n",
      "508        High   101778     7.340000   27.070000         268.801078   \n",
      "512      Medium  1326801   262.010010  120.339996         -54.070458   \n",
      "362      Medium  1633917   241.850006   71.220001         -70.551995   \n",
      "452      Medium  1108524   228.460007  132.589996         -41.963586   \n",
      "450        High   829224    98.580002   99.199997           0.628926   \n",
      "429        High  1283699   128.419998  140.000000           9.017289   \n",
      "471        High    27419   183.570007  149.039993         -18.810270   \n",
      "430        High  1318605   279.936676  123.180000         -55.997191   \n",
      "428      Medium   100517    39.939999   37.700001          -5.608407   \n",
      "465        High  1744489   170.970001   86.879997         -49.184069   \n",
      "189        High  1065088    58.470001   41.470001         -29.074739   \n",
      "\n",
      "    volatility  50_day_SMA cumulative_return  \n",
      "161   0.013298  156.389600          0.579765  \n",
      "147   0.015894  146.813401         -0.140894  \n",
      "6     0.032561   13.750600         -0.244656  \n",
      "10    0.019274  143.075999         -0.031385  \n",
      "133   0.018294   35.125800          0.105474  \n",
      "660   0.032398   38.121400          0.245274  \n",
      "130   0.013554  238.320800         -0.002726  \n",
      "121   0.017000   57.558000          0.326117  \n",
      "86    0.022012   29.759500         -0.080536  \n",
      "273   0.012049   97.451199          0.105517  \n",
      "235   0.025501   37.856800         -0.346923  \n",
      "231   0.017199  359.515798          0.249882  \n",
      "508   0.033453   29.454800          2.688011  \n",
      "512   0.031837  113.209200         -0.540705  \n",
      "362   0.030989   78.270200         -0.705520  \n",
      "452   0.024839  145.095600         -0.419636  \n",
      "450   0.018808   96.071000          0.006289  \n",
      "429   0.016618  145.478800          0.090173  \n",
      "471   0.023134  157.749400         -0.188103  \n",
      "430   0.038380  179.794800         -0.559972  \n",
      "428   0.031416   41.972600         -0.056084  \n",
      "465   0.019923   95.233000         -0.491841  \n",
      "189   0.022532   42.343200         -0.290747  \n",
      "\n",
      "[23 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "companies = [\"Walt Disney Co\", \"American Airlines Group Inc\", \"Apple Inc\", \"eBay Inc\", \"Goldman Sachs Group Inc\", \n",
    "             \"Meta Platforms Inc\", \"Starbucks Corp\", \"PayPal Holdings Inc\", \"United Airlines Holdings Inc\", \n",
    "             \"Bath & Body Works Inc\", \"Abbvie Inc\", \"Alexandria Real Estate Equities Inc\", \n",
    "             \"Becton Dickinson and Co\", \"Brown & Brown Inc\", \"Duke Energy Corp\", \"T-Mobile US Inc\",\n",
    "             \"Marathon Oil Corp\", \"Chipotle Mexican Grill Inc\", \"Target Corp\", \n",
    "             \"General Motors Co\", \"Salesforce Inc\", \"Tesla Inc\", \"Bank of America Corp\"]\n",
    "\n",
    "sample_companies = merged_df[merged_df[\"name\"].isin(companies)]\n",
    "print(sample_companies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, we also extracted some general S&P 500 data from yfinance, ranging from the dates of 2/1/21 and 12/31/22 for the same reason. we are pulling this data so that we can compare stock performance of the individual companies to the overall S&P 500 in the same time range. sp500 will be our 3rd dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Start Price    End Price  Rate of Change\n",
      "Date                                                \n",
      "2021-02-01  3731.169922  3773.860107        1.144150\n",
      "2021-02-02  3791.840088  3826.310059        0.909057\n",
      "2021-02-03  3840.270020  3830.169922       -0.263005\n",
      "2021-02-04  3836.659912  3871.739990        0.914339\n",
      "2021-02-05  3878.300049  3886.830078        0.219942\n",
      "...                 ...          ...             ...\n",
      "2022-12-23  3815.110107  3844.820068        0.778745\n",
      "2022-12-27  3843.340088  3829.250000       -0.366610\n",
      "2022-12-28  3829.560059  3783.219971       -1.210063\n",
      "2022-12-29  3805.449951  3849.280029        1.151771\n",
      "2022-12-30  3829.060059  3839.500000        0.272650\n",
      "\n",
      "[484 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "sp500data = yf.download('^GSPC', start = '2021-02-01', end = '2022-12-31', progress = False)\n",
    "sp500 = pd.DataFrame({\n",
    "    'Date': sp500data.index,\n",
    "    'Start Price': sp500data['Open'],\n",
    "    'End Price': sp500data['Close'],\n",
    "    'Rate of Change': ((sp500data['Close'] - sp500data['Open']) / sp500data['Open']) * 100 })\n",
    "\n",
    "sp500.set_index('Date', inplace = True)\n",
    "print(sp500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### __Data Description__\n",
    "\n",
    "1) Where can your raw source data be found, if applicable? Provide a link to the raw data (hosted on Github, in a Cornell Google Drive or Cornell Box).\n",
    "\n",
    "- We have 3 main datasets: 1 main dataset (merged_df), 1 \"sample\" dataset that selects 23 rows from this main dataset (sample_companies), and 1 additional dataset (sp500). Our raw data for the first 2 datasets can be found on Kaggle, here: https://github.com/anushabishayee/info2950_finalproject/blob/main/raw%20data/esg%20kaggle%20csv%20link and the actual csv is here: https://github.com/anushabishayee/info2950_finalproject/blob/main/esg_data.csv.\n",
    "- Even more specifically, the Kaggle author states that they pulled the data for their csv from multiple APIs, like ESG Enterprise, a publicly-available API. They grabbed financial and company data from Finnhub. 3 of these links can be found here: https://github.com/anushabishayee/info2950_finalproject/blob/main/raw%20data/esg%20kaggle%20source%20data%20links.\n",
    "- The finance data that the 3rd dataset is comprised of, and the finance data that is joined to the 1st and 2nd datasets is found in the yfinance library in Python (Yahoo Finance data, https://github.com/anushabishayee/info2950_finalproject/blob/main/raw%20data/yfinance%20links).\n",
    "\n",
    "***\n",
    "\n",
    "2) If people are involved, were they aware of the data collection and if so, what purpose did they expect the data to be used for? \n",
    "\n",
    "- Individuals are not involved in the data directly, as each observation corresponds to an entire company.\n",
    "\n",
    "***\n",
    "\n",
    "3) What preprocessing was done, and how did the data come to be in the form that you are using?\n",
    "\n",
    "- Our preprocessing of these datasets is detailed above. Generally, we imported the yfinance library, downloaded the Kaggle csv with the company ESG data, cleaned the dataset for NaNs and unneeded values, and reformatted some date values for ease of manipulation. Then, we joined the ESG data to the yfinance stock data, matching on company ticker (we created 4 new stock metric columns), and dropped NaNs for the creation of merged_df. For the 2nd dataset (sample_companies), we filtered 23 specific companies from this main dataset. For the 3rd dataset, we extracted the data straight from the yfinance library, and calculated a rate of change variable for the stock change as well for sp500.\n",
    "- For the Kaggle csv, the author notes that they used company stock ticker as a unique identifier, then pulled and collated data from various APIs. in specific, they utilized ESG Enterprise (https://www.esgenterprise.com/), a publicly-available API, and pulled their ratings methodology from https://app.esgenterprise.com/uploads/ESG-Enterprise-Risk-Ratings-MethodologyV3.pdf. They grabbed financial and company data from Finnhub (https://finnhub.io/). \n",
    "\n",
    "***\n",
    "\n",
    "4) What processes might have influenced what data was observed and recorded and what was not?\n",
    "\n",
    "- For the ESG data, the Kaggle author of the csv specifically mentioned that only mid/large-cap companies are included, so this influences the specific companies that are recorded in the initial data - smaller companies (that also might have an ESG score) will not be 'observed' here. The author pulled data from ESG Enterprise and Finnhub, so any companies that do not have data available there will not be observed in the dataset. We also dropped any company that had a NaN or blank column value for the ESG columns, and dropped any company that didn't have stock data available in Yahoo Finance (or had NaNs for any specific finance column).\n",
    "\n",
    "***\n",
    "\n",
    "5) Who funded the creation of the dataset?\n",
    "\n",
    "- We created these 3 analysis-ready datasets from two data sources: a 'Public Company ESG Ratings Dataset' Kaggle dataset from user Alistair King (https://www.kaggle.com/alistairking), a New York-based Kaggle Datasets Grandmaster, as well as the yfinance Python library, created by Ran Aroussi (https://aroussi.com/) as a way around the 2017 Yahoo Finance API deprecation. It is unclear if these datasets were 'funded', but their organization and accumulation were spearheaded by the two aforementioned people, respectively.\n",
    "\n",
    "***\n",
    "\n",
    "6) Why was this dataset created? \n",
    "- We formulated our main analysis-ready dataset (merged_df) to examine associations between some of the largest USD-utilizing companies' ESG scores and their stock performances (as well as industry-specific analyses). Then, we formulated our sample dataset (sample_companies) so that we could take a look at some company-level analyses of the general data and research question (620 companies are kinda hard to visualize simultaneously). Finally, we formulated the sp500 dataset so that we could contrast company stock performance from the specified range of 2/1/21 - 12/31/22 to the overall performance of the S&P 500. (the rationale for the range of 2/1/21 - 12/31/22 is mentioned above, its due to the fact that most companies have a 'last processing date' of February 2022 - Novermber 2022 for their ESG score.) \n",
    "- The original ESG csv was created and uploaded by Kaggle user Alistair King, perhaps for personal enrichment or curiosity (they do have a Kaggle Datasets Grandmaster rank, so perhaps they just enjoy creating and uploading datasets). The original yfinance Python library was created by Ran Aroussi to have a simple way to download historical market data from Yahoo Finance, due to the Yahoo Finance API deprecation.\n",
    "\n",
    "***\n",
    "\n",
    "7) What are the observations (rows) and attributes (columns)?\n",
    "- For the S&P 500 dataset (sp500), the rows each correspond to a specific date where the S&P 500 was measured, within the range from 2/1/21 - 12/31/22. The columns for this dataset are Start Price, End Price, and Rate of Change (aka the starting price of the S&P 500 when the US market opened on a specific day, the ending price of the S&P 500 when the US market closed on the same specific day, and the percentage change that this stock exhibited between the start and close times of that specific day (100 * (end price - start price) / start price)).\n",
    "\n",
    "- For both the merged_df and sample_companies dataset, each row corresponds to an unique, mid- to large-cap company that is publicly-traded and utilizes USD. merged_df, our main dataset, has 620 companies, while sample_companies has 23 companies for now. merged_df and sample_companies have the same columns, they are:\n",
    "+ ticker - a unique combo of letters and numbers that represent a particular stock\n",
    "+ name - the official name of the company\n",
    "+ currency - the currency the company is traded in (this was filtered to only be USD)\n",
    "+ exchange - what market the company is exchanged on\n",
    "+ industry - the type of output the company produces\n",
    "+ logo - a link to the company logo, potentially for joining with other datasets (MIGHT BE DROPPED LATER)\n",
    "+ weburl - a link to the company website, potentially for joining with other datasets or scraping for text sentiment analysis (MIGHT BE DROPPED LATER)\n",
    "+ environment_grade - a letter score given to the company that measures how well it complies to environmental standards, ranging from AAA being the best to CCC being the worst\n",
    "+ environment_level - a categorical classification of a company's overall environmental performance (low, medium, high)\n",
    "+ social_grade - a letter score given to the company that measures how well it complies to social standards, ranging from AAA being the best to CCC being the worst\n",
    "+ social_level - a categorical classification of a company's overall social performance (low, medium, high)\n",
    "+ governance_grade - a letter score given to the company that measures how well it complies to governance standards, ranging from AAA being the best to CCC being the worst\n",
    "+ governance_level - a categorical classification of a company's overall governance performance (low, medium, high)\n",
    "+ environment_score - a numerical measure of how well a company performs on environment-related factors, ranging from 0-1000\n",
    "+ social_score - a numerical measure of how well a company performs on social-related factors, ranging from 0-1000\n",
    "+ governance_score - a numerical measure of how well a company performs on governance-related factors, ranging from 0-1000\n",
    "+ total_score - a numerical measure of how well a company performs on environment, social, and governance-related factors, ranging from 0-1500]\n",
    "+ cik - central index key, a unique identifier assigned by the SEC to any company that files documents with the SEC (MIGHT BE DROPPED LATER)\n",
    "the following columns are ones that we created, using the yfinance data:\n",
    "+ percent_change - the percent change in the company stock price from close time on 2/1/21 to close time on 12/31/22 (100 * (end price - start price) / start price))\n",
    "+ start_close - the closing price of the company stock on 2/1/21\n",
    "+ end_close - the closing price of the company stock on 12/31/22\n",
    "+ volatility - standard deviation of daily returns of the company stock, aka the percentage change in the stock price from day to day (indicator of how much stock price fluctuates in a given period, higher volatility is riskier, lower volatility has more stability). specifically, daily return is calculated by closing price on day x+1 - closing price on day x divided by closing price on day x, so all daily returns in the time period 2/1/21-12/31/22 are calculated for the specific company stock, and then the standard deviation is taken to get the volatility\n",
    "+ 50_day_SMA - 50 day simple moving average, or the sum of closing price of a company stock for the last 50 days before 12/31/22, divided by 50 (if current stock price is above the 50-day SMA, the company is in uptrend, and vice versa)\n",
    "+ cumulative_return - cumulative return of the company stock over the entire period ((close price on 12/31/22 / close price on 2/1/21) - 1), positive values represent returns, and negative values represnt losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### __Data Limitations__\n",
    "\n",
    "1. ESG is typically evaluated annually, which might mean the scores don't reflect the most accurate performance of the company. Thus, when considering short-term impact of the company's ESG and other policies, it's likely that the policy change will affect stocks directly, but might not be reflected in the company's ESG rating.\n",
    "\n",
    "2. ESG is a constant value that is gathered from different days for each company in 2022, though stock prices for these companies change over time. We cannot perform any time-series analyses with ESG due to this fact.\n",
    "\n",
    "3. Due to the nature of the Kaggle csv and yfinance data, our data is restricted to the variable types of stock data, industry type, company name, and ESG score - which actually does help us narrow down the scope of our research question (so can be a positive limitation).\n",
    "\n",
    "4. Some specific data from the yfinance library is missing - we had to drop all companies that didn't have the specified data we wanted in our specified time range. We also had to drop all companies from the original ESG csv that had missing or blank data. Additionally, we filtered our original ESG dataframe to be just companies traded in USD, so we can't do any inter-country comparison (although this also helps us narrow the scope of our project). \n",
    "\n",
    "5. For our sample dataset (companies_sample), the current 23 companies were chosen manually by us going through the list of companies and selecting well-known ones from differing industries. This sample is not fully representative of all USD-using companies with ESG ratings, and we most likely will have to re-evaluate how we choose this sample to examine company-specific data (maybe we will choose the 25 companies with the highest average stock prices, grouping by industry). We may also consider random sampling to expand the representativeness of our sample dataset. Since we are exploring potential connections between ESG ratings and company stock performance, we may need to sample not only by industry but also by ESG rating levels to ensure a more balanced and comprehensive analysis of the different ESG performance tiers (for our sample dataset).\n",
    "\n",
    "6. We currently plan on comparing the rate of change of the sample stocks (in companies_sample) to the S&P 500's rate of change. We also plan on taking a look at volatility, cumulative returns, and the 50 day simple moving average, but other measures of stock performance might provide more valuable insights. Due to the last processing date of the ESG scores for the companies, we also restricted our stock data to be from 2/1/21 - 12/31/22, which poses a limitation on the amount of analyses we can garner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### __Exploratory Data Analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### part one - exploring different average environmental, social, governance, and total ESG scores by industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best average ESG scores\n",
      "                    Industry  Average Total ESG Score\n",
      "46                 Utilities              1190.633333\n",
      "44                   Tobacco              1165.000000\n",
      "24  Industrial Conglomerates              1157.000000\n",
      "17                    Energy              1154.764706\n",
      "34                 Packaging              1146.666667\n",
      "20             Food Products              1136.307692\n",
      "\n",
      "worst average ESG scores\n",
      "                         Industry  Average Total ESG Score\n",
      "32                Metals & Mining               615.000000\n",
      "0             Aerospace & Defense               633.000000\n",
      "23  Hotels, Restaurants & Leisure               746.000000\n",
      "26               Leisure Products               780.000000\n",
      "18                        Energy                819.000000\n",
      "2                        Airlines               838.714286\n"
     ]
    }
   ],
   "source": [
    "avg_esg_by_industry = merged_df.groupby('industry')['total_score'].mean().reset_index()\n",
    "avg_esg_by_industry.columns = ['Industry', 'Average Total ESG Score']\n",
    "\n",
    "avg_esg_by_industry = avg_esg_by_industry.sort_values(by = 'Average Total ESG Score', ascending = False)\n",
    "print(\"best average ESG scores\")\n",
    "print(avg_esg_by_industry.head(6))\n",
    "\n",
    "avg_esg_by_industry = avg_esg_by_industry.sort_values(by = 'Average Total ESG Score', ascending = True)\n",
    "print(\"\\nworst average ESG scores\")\n",
    "print(avg_esg_by_industry.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 10))\n",
    "sns.barplot(x = 'Average Total ESG Score', y = 'Industry', data = avg_esg_by_industry, color = \"#b97df3\")\n",
    "plt.title('Average Total ESG Score by Industry', horizontalalignment = 'center', fontsize = 16, fontweight = 'bold', )\n",
    "plt.xlabel('Average Total ESG Score', fontsize = 14, fontweight = 'bold')\n",
    "plt.ylabel('Industry Name', fontsize = 14, fontweight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interestingly (and somewhat predictably) - the industries with the lowest ESG scores are Metals & Mining, Aerospace & Defense, Diversified Consumer Services, Hotels, Restaurants & Leisure, Leisure Products, Auto Components, Airlines, and Automobiles. the industries with the highest ESG scores are Utilities, Tobacco, Industrial Conglomerates, Packaging, and Energy. \n",
    "\n",
    "future steps: sort different industries by just Environmental score, just Social score, and just Governance score to see if these differ significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_columns = ['environment_score', 'social_score', 'governance_score', 'total_score']\n",
    "\n",
    "corr_matrix = merged_df[score_columns].corr()\n",
    "\n",
    "plt.figure(figsize = (10, 9))\n",
    "sns.heatmap(corr_matrix, annot = True, fmt = \".2f\", cmap = 'coolwarm', square = True, cbar_kws = {\"shrink\": .8}, linewidths = 0.5)\n",
    "plt.title('Correlation Matrix of ESG Scores', fontsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use yfinance to pull stock information of selected stocks.\n",
    "esg.loc[:, 'ticker'] = esg['ticker'].astype(str)\n",
    "esg.loc[:, 'name'] = esg['name'].astype(str)\n",
    "tickers = esg['ticker'].tolist()\n",
    "\n",
    "#Add Start Price, End Price, and Rate of Change (%) of each company to the dataset esg\n",
    "esg.loc[:, 'Start Price'] = None\n",
    "esg.loc[:, 'End Price'] = None\n",
    "esg.loc[:, 'Rate of Change (%)'] = None\n",
    "\n",
    "# Loop through each row of the DataFrame to get stock information for each company\n",
    "for index, row in esg.iterrows():\n",
    "    ticker = row['ticker']\n",
    "    \n",
    "    # Download stock data for 2023\n",
    "    data = yf.download(ticker, start='2021-04-01',end='2022-04-01')\n",
    "    \n",
    "    # Ensure data exists for the given period\n",
    "    if not data.empty:\n",
    "        start_price = data['Adj Close'].iloc[0]\n",
    "        end_price = data['Adj Close'].iloc[-1]\n",
    "        rate_of_change = ((end_price - start_price) / start_price) * 100\n",
    "        \n",
    "        # Add the stock data to the relevant columns in the DataFrame using .loc[]\n",
    "        esg.loc[index, 'Start Price'] = start_price\n",
    "        esg.loc[index, 'End Price'] = end_price\n",
    "        esg.loc[index, 'Rate of Change (%)'] = rate_of_change\n",
    "\n",
    "print(esg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning to only have certain companies that represent a variety of industries\n",
    "\n",
    "companies = [\"Walt Disney Co\", \"American Airlines Group Inc\", \"Apple Inc\", \"eBay Inc\", \"Goldman Sachs Group Inc\", \n",
    "             \"Meta Platforms Inc\", \"Starbucks Corp\", \"PayPal Holdings Inc\", \"United Airlines Holdings Inc\", \n",
    "             \"Bath & Body Works Inc\", \"Abbvie Inc\", \"Alexandria Real Estate Equities Inc\", \n",
    "             \"Becton Dickinson and Co\", \"Brown & Brown Inc\", \"Duke Energy Corp\", \"T-Mobile US Inc\",\n",
    "             \"Marathon Oil Corp\", \"Chipotle Mexican Grill Inc\", \"Target Corp\", \n",
    "             \"General Motors Co\", \"Salesforce Inc\", \"Tesla Inc\", \"Bank of America Corp\"]\n",
    "\n",
    "relevant_esg = esg[esg[\"name\"].isin(companies)]\n",
    "print(relevant_esg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use yfinance to pull stock information of selected stocks.\n",
    "relevant_esg.loc[:, 'ticker'] = relevant_esg['ticker'].astype(str)\n",
    "relevant_esg.loc[:, 'name'] = relevant_esg['name'].astype(str)\n",
    "tickers = relevant_esg['ticker'].tolist()\n",
    "#Add Start Price, End Price, and Rate of Change (%) of each company to the dataset relevent.esg\n",
    "relevant_esg.loc[:, 'Start Price'] = None\n",
    "relevant_esg.loc[:, 'End Price'] = None\n",
    "relevant_esg.loc[:, 'Rate of Change (%)'] = None\n",
    "\n",
    "# Loop through each row of the DataFrame to get stock information for each company\n",
    "for index, row in relevant_esg.iterrows():\n",
    "    ticker = row['ticker']\n",
    "    \n",
    "    # Download stock data for 2023\n",
    "    data = yf.download(ticker, start='2021-04-01',end='2022-04-01')\n",
    "    \n",
    "    # Ensure data exists for the given period\n",
    "    if not data.empty:\n",
    "        start_price = data['Adj Close'].iloc[0]\n",
    "        end_price = data['Adj Close'].iloc[-1]\n",
    "        rate_of_change = ((end_price - start_price) / start_price) * 100\n",
    "        \n",
    "        # Add the stock data to the relevant columns in the DataFrame using .loc[]\n",
    "        relevant_esg.loc[index, 'Start Price'] = start_price\n",
    "        relevant_esg.loc[index, 'End Price'] = end_price\n",
    "        relevant_esg.loc[index, 'Rate of Change (%)'] = rate_of_change\n",
    "\n",
    "print(relevant_esg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Start Price, End Price, and Rate of Change (%) of each company to the dataset esg\n",
    "esg.loc[:, 'ticker'] = esg['ticker'].astype(str)\n",
    "esg.loc[:, 'name'] = esg['name'].astype(str)\n",
    "tickers = esg['ticker'].tolist()\n",
    "\n",
    "# Add columns for Start Price, End Price, and Rate of Change (%)\n",
    "esg['Start Price'] = None\n",
    "esg['End Price'] = None\n",
    "esg['Rate of Change (%)'] = None\n",
    "\n",
    "# Loop through each row of the DataFrame to get stock information for each company\n",
    "for index, row in esg.iterrows():\n",
    "    ticker = row['ticker']\n",
    "    \n",
    "    # Skip rows where the ticker is not valid\n",
    "    if ticker == 'nan' or ticker.strip() == \"\":\n",
    "        continue\n",
    "    \n",
    "    # Download stock data for the given ticker\n",
    "    data = yf.download(ticker, start='2021-04-01', end='2022-04-01')\n",
    "    \n",
    "    # Ensure data exists for the given period\n",
    "    if not data.empty:\n",
    "        start_price = data['Adj Close'].iloc[0]\n",
    "        end_price = data['Adj Close'].iloc[-1]\n",
    "        rate_of_change = ((end_price - start_price) / start_price) * 100\n",
    "        \n",
    "        # Add the stock data to the relevant columns in the DataFrame\n",
    "        esg.loc[index, 'Start Price'] = start_price\n",
    "        esg.loc[index, 'End Price'] = end_price\n",
    "        esg.loc[index, 'Rate of Change (%)'] = rate_of_change\n",
    "\n",
    "#Remove companies with missing value on stock information:\n",
    "esg.dropna(subset=['Start Price', 'End Price', 'Rate of Change (%)'], inplace=True)\n",
    "\n",
    "#show the first 15 rows of cleaned esg\n",
    "print(esg.iloc[0:15,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### part two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Violin Plot: Governance Score by Environment Level\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(data=esg, x='environment_level', y='governance_score', palette='muted')\n",
    "plt.title('Governance Scores by Environment Level', fontsize=16)\n",
    "plt.xlabel('Environment Level')\n",
    "plt.ylabel('Governance Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of total_score\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(esg['total_score'], kde=True, bins=20)\n",
    "plt.title('Distribution of Total ESG Scores', fontsize=16)\n",
    "plt.xlabel('Total Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot of Total Scores by Total Grade\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=esg, x='total_grade', y='total_score', palette='Set2')\n",
    "plt.title('Total ESG Scores by Grade', fontsize=16)\n",
    "plt.xlabel('Total Grade')\n",
    "plt.ylabel('Total Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter Plot: Rate of Change vs. Total ESG Score\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=esg, x='total_score', y='Rate of Change (%)', hue='total_grade', palette='coolwarm', s=100)\n",
    "plt.title('Rate of Change (%) vs. Total ESG Score', fontsize=16)\n",
    "plt.xlabel('Total ESG Score')\n",
    "plt.ylabel('Rate of Change (%)')\n",
    "plt.legend(title=\"ESG Grade\", loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot: Rate of Change by Environment Grade\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=esg, x='environment_grade', y='Rate of Change (%)', palette='Set2')\n",
    "plt.title('Rate of Change (%) by Environmental Grade', fontsize=16)\n",
    "plt.xlabel('Environment Grade')\n",
    "plt.ylabel('Rate of Change (%)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Violin Plot: Rate of Change by Social Grade\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(data=esg, x='social_grade', y='Rate of Change (%)', palette='muted')\n",
    "plt.title('Rate of Change (%) by Social Grade', fontsize=16)\n",
    "plt.xlabel('Social Grade')\n",
    "plt.ylabel('Rate of Change (%)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix=esg[['environment_score', 'social_score', 'governance_score', 'Rate of Change (%)']].corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix Between ESG Subscores and Stock Price Change')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "correlation_environment = esg['environment_score'].corr(esg['Rate of Change (%)'])\n",
    "correlation_social = esg['social_score'].corr(esg['Rate of Change (%)'])\n",
    "correlation_governance = esg['governance_score'].corr(esg['Rate of Change (%)'])\n",
    "\n",
    "print(f'Correlation between Environmental Score and Rate of Change (%): {correlation_environment:.3f}')\n",
    "print(f'Correlation between Social Score and Rate of Change (%): {correlation_social:.3f}')\n",
    "print(f'Correlation between Governance Score and Rate of Change (%): {correlation_governance:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for each ESG subscore\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(esg['environment_score'].dropna(), bins=20, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Environmental Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Environmental Scores')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(esg['social_score'].dropna(), bins=20, color='lightgreen', edgecolor='black')\n",
    "plt.xlabel('Social Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Social Scores')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(esg['governance_score'].dropna(), bins=20, color='lightcoral', edgecolor='black')\n",
    "plt.xlabel('Governance Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Governance Scores')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_scores = relevant_esg['environment_score'].values\n",
    "soc_scores = relevant_esg['social_score'].values\n",
    "gov_scores = relevant_esg['governance_score'].values\n",
    "filtered_companies = relevant_esg['name'].values\n",
    "\n",
    "x = np.arange(len(filtered_companies))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x, env_scores, color='skyblue', label='Environmental Score')\n",
    "plt.bar(x, soc_scores, bottom=env_scores, color='lightgreen', label='Social Score')\n",
    "plt.bar(x, gov_scores, bottom=env_scores + soc_scores, color='lightcoral', label='Governance Score')\n",
    "\n",
    "plt.xticks(x, filtered_companies, rotation=45, ha='right')\n",
    "plt.xlabel('Company')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Stacked Bar Chart of ESG Scores by Company')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = yf.download(list(relevant_esg['ticker']), start='2023-01-01', end='2024-01-01')['Adj Close']\n",
    "        \n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "for ticker in data.columns:\n",
    "    plt.plot(data.index, data[ticker], label=ticker)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Adjusted Closing Price (USD)')\n",
    "plt.title('Stock Price Changes Over Time for Selected Companies')\n",
    "plt.legend(loc='upper left', fontsize='small')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = relevant_esg[['environment_score', 'social_score','governance_score']]   # Independent variables\n",
    "y = relevant_esg['Rate of Change (%)']   # Dependent variable\n",
    "\n",
    "model = LinearRegression().fit(X, y)\n",
    "print(f\"Environmental Score Coefficient: {model.coef_[0]}\")\n",
    "print(f\"Social Score Coefficient: {model.coef_[1]}\")\n",
    "print(f\"Governance Score Coefficient: {model.coef_[2]}\")\n",
    "print(f\"Intercept: {model.intercept_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Coefficients Interpretation\n",
    "\n",
    "For every 1-unit increase in the Environmental score (assuming all other factors remain constant), the stock return is expected to increase by 0.1079 units. The positive coefficient suggests that higher Environmental scores are associated with better stock performance (or higher returns).\n",
    "\n",
    "For every 1-unit increase in the Social score (with other variables constant), the stock return is expected to decrease by 0.0237 units. The negative coefficient indicates that better Social scores might be associated with lower stock performance, but generally, since the coefficient is so close to 0, Social scores seem to have little impact on stock performance.\n",
    "\n",
    "For every 1-unit increase in the Governance score (with other variables constant), stock return is expected to decrease by 0.0750 units. This negative coefficient suggests that improvements in governance (e.g., stricter regulation or more ethical practices) are associated with slightly lower stock returns. This could imply that governance improvements come at a financial cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esg_corr = relevant_esg.loc[:, ['environment_score', 'social_score',\n",
    "                                'governance_score']] \n",
    "print(esg_corr.corr(numeric_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(esg_corr.cov(numeric_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation and Covariance \n",
    "\n",
    "Let's observe the correlation/covariance between the different ESG scores themselves! All the correlations are positive, meaning that there is positive correlation between the scores (when one increases, the other does too). Governance and Environment have a particularly strong correlation, suggesting that companies who invest in environmental factors likely also care about governance (or perhaps some government regulations align with environmental issues). On the other hand, social factors seem to have just a moderately positive relationship with both the other variables.\n",
    "\n",
    "While the covariance values agree with these claims, it's interesting to note how large the variance is for Environmental Scores (20758.63). This suggests that there is a large spread in environmental performance among the companies we chose, while governance scores are much more consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### __Questions for Reviewers__\n",
    "\n",
    "1. Does it seem like we have enough columns in merged_df and sample_companies to satisfy the complexity requirement for the project? Does our research question also seem complex enough?\n",
    "   \n",
    "2. Is there any advice or recommended steps to follow in creating our sample_companies dataset from the population? Should we try doing a random sample of 1 company from each industry? (how many industries should we actually take samples from to have a meaningful project then?) We were also thinking of stratifying our sample to include all of the different ESG grades. Another option is picking the biggest company (measured probably by end_price) from each industry - do any of these methods sound more robust than the other ones? (as a reminder - we have one big dataset, merged_df with 600 companies that we did some EDA with, but we also want to include a smaller sample dataset so we can look at some individual companies as well. the purpose of this smaller dataset would be then to observe trends on a company-level, and study the metrics of our research question on this smaller scale). Also - would it be valid to maybe pick a few companies within the same industry (like the 20 biggest companies in the technology industry) for this sample instead? We could then look at how ESG ratings and stock performance differs within an industry itself. \n",
    "   \n",
    "3. How many visualizations (or like data analysis chunks) recommended for the final project? (ballpark range would be helpful, or will we get to look at some examples of a completed project later?)\n",
    "\n",
    "4. Do the visualizations we currently have seem like they're on the right path for the final phases?\n",
    "   \n",
    "5. Regarding the visualizations and chunks we have made for our EDA so far: should we further explore these specific visualizations more in depth? Or should we expand our DA to other variables in the datasets that we maybe haven't used yet?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info2950",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
