{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 2950: Phase IV\n",
    "#### Group Members: Anusha Bishayee, Katheryn Ding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### __Introduction:__  \n",
    "\n",
    "#### How do ESG score and stock performance (price) align across different industries? What associations can we find between company industry, stock performance, and ESG ratings?\n",
    "#### note: ESG score refers to a quantiative metric measuring a company's environmental, social, and governance performance; 'environmental' pertains to aspects like waste management and energy emissions, 'social' pertains to aspects like customer satisfaction and DEI in the workplace, and 'governance' pertains to aspects like operating efficiencies and risk management. ESG scores are typically examined by independent investors, business analysts, and even competitior companies to assess risk or opportunities associated with a specific company's practices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### __Data Collection and Cleaning:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import yfinance as yf\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the data cleaning work that produces these csvs is found here: https://github.com/anushabishayee/info2950_finalproject/blob/main/data_cleaning.ipynb.\n",
    "\n",
    "Generally, we imported the yfinance library, downloaded the Kaggle csv with the company ESG data, cleaned the dataset for NaNs and unneeded values, and reformatted some date values for ease of manipulation. Then, we joined the ESG data to the yfinance stock data, matching on company ticker (we created 4 new stock metric columns), and dropped NaNs for the creation of merged_df. For the 2nd dataset (sample_companies), we stratified and randomly selected 30 specific companies from this main dataset (specific methodology is outlined above in the 'Data Collection and Cleaning' section. For the 3rd dataset, we extracted the data straight from the yfinance library, and calculated a rate of change variable for the stock change as well for sp500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_esg_by_industry = pd.read_csv(\"avg_esg_by_industry.csv\")\n",
    "sp500 = pd.read_csv(\"sp500.csv\")\n",
    "merged_df = pd.read_csv(\"merged_df.csv\")\n",
    "sample_companies = pd.read_csv(\"sample_companies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### __Data Description__\n",
    "1) Where can your raw source data be found, if applicable? Provide a link to the raw data (hosted on Github, in a Cornell Google Drive or Cornell Box).\n",
    "\n",
    "- We have 3 main datasets: 1 main dataset (merged_df), 1 \"sample\" dataset that selects 30 rows from this main dataset (sample_companies), and 1 additional dataset (sp500). Our raw data for the first 2 datasets can be found on Kaggle, here: https://github.com/anushabishayee/info2950_finalproject/blob/main/raw%20data/esg%20kaggle%20csv%20link and the actual csv is here: https://github.com/anushabishayee/info2950_finalproject/blob/main/esg_data.csv.\n",
    "- Even more specifically, the Kaggle author states that they pulled the data for their csv from multiple APIs, like ESG Enterprise, a publicly-available API. They grabbed financial and company data from Finnhub. 3 of these links can be found here: https://github.com/anushabishayee/info2950_finalproject/blob/main/raw%20data/esg%20kaggle%20source%20data%20links.\n",
    "- The finance data that the 3rd dataset is comprised of, and the finance data that is joined to the 1st and 2nd datasets is found in the yfinance library in Python (Yahoo Finance data, https://github.com/anushabishayee/info2950_finalproject/blob/main/raw%20data/yfinance%20links).\n",
    "\n",
    "***\n",
    "\n",
    "2) If people are involved, were they aware of the data collection and if so, what purpose did they expect the data to be used for?\n",
    "\n",
    "- Individuals are not involved in the data directly, as each observation corresponds to an entire company.\n",
    "\n",
    "***\n",
    "\n",
    "3) What preprocessing was done, and how did the data come to be in the form that you are using?\n",
    "\n",
    "- Our preprocessing of these datasets is detailed above. Generally, we imported the yfinance library, downloaded the Kaggle csv with the company ESG data, cleaned the dataset for NaNs and unneeded values, and reformatted some date values for ease of manipulation. Then, we joined the ESG data to the yfinance stock data, matching on company ticker (we created 4 new stock metric columns), and dropped NaNs for the creation of merged_df. For the 2nd dataset (sample_companies), we stratified and randomly selected 30 specific companies from this main dataset (specific methodology is outlined above in the 'Data Collection and Cleaning' section. For the 3rd dataset, we extracted the data straight from the yfinance library, and calculated a rate of change variable for the stock change as well for sp500.\n",
    "- For the Kaggle csv, the author notes that they used company stock ticker as a unique identifier, then pulled and collated data from various APIs. in specific, they utilized ESG Enterprise (https://www.esgenterprise.com/), a publicly-available API, and pulled their ratings methodology from https://app.esgenterprise.com/uploads/ESG-Enterprise-Risk-Ratings-MethodologyV3.pdf. They grabbed financial and company data from Finnhub (https://finnhub.io/).\n",
    "\n",
    "***\n",
    "\n",
    "4) What processes might have influenced what data was observed and recorded and what was not?\n",
    "\n",
    "- For the ESG data, the Kaggle author of the csv specifically mentioned that only mid/large-cap companies are included, so this influences the specific companies that are recorded in the initial data - smaller companies (that also might have an ESG score) will not be 'observed' here. The author pulled data from ESG Enterprise and Finnhub, so any companies that do not have data available there will not be observed in the dataset. We also dropped any company that had a NaN or blank column value for the ESG columns, and dropped any company that didn't have stock data available in Yahoo Finance (or had NaNs for any specific finance column).\n",
    "\n",
    "***\n",
    "\n",
    "5) Who funded the creation of the dataset?\n",
    "\n",
    "- We created these 3 analysis-ready datasets from two data sources: a 'Public Company ESG Ratings Dataset' Kaggle dataset from user Alistair King (https://www.kaggle.com/alistairking), a New York-based Kaggle Datasets Grandmaster, as well as the yfinance Python library, created by Ran Aroussi (https://aroussi.com/) as a way around the 2017 Yahoo Finance API deprecation. It is unclear if these datasets were 'funded', but their organization and accumulation were spearheaded by the two aforementioned people, respectively.\n",
    "\n",
    "***\n",
    "\n",
    "6) Why was this dataset created?\n",
    "- We formulated our main analysis-ready dataset (merged_df) to examine associations between some of the largest USD-utilizing companies' ESG scores and their stock performances (as well as industry-specific analyses). Then, we formulated our sample dataset (sample_companies) so that we could take a look at some company-level analyses of the general data and research question (620 companies are kinda hard to visualize simultaneously). Finally, we formulated the sp500 dataset so that we could contrast company stock performance from the specified range of 2/1/21 - 12/31/22 to the overall performance of the S&P 500. (the rationale for the range of 2/1/21 - 12/31/22 is mentioned above, it's due to the fact that most companies have a 'last processing date' of February 2022 - Novermber 2022 for their ESG score.)\n",
    "- The original ESG csv was created and uploaded by Kaggle user Alistair King, perhaps for personal enrichment or curiosity (they do have a Kaggle Datasets Grandmaster rank, so perhaps they just enjoy creating and uploading datasets). The original yfinance Python library was created by Ran Aroussi to have a simple way to download historical market data from Yahoo Finance, due to the Yahoo Finance API deprecation.\n",
    "\n",
    "***\n",
    "\n",
    "7) What are the observations (rows) and attributes (columns)?\n",
    "- For the S&P 500 dataset (sp500), the rows each correspond to a specific date where the S&P 500 was measured, within the range from 2/1/21 - 12/31/22. The columns for this dataset are Start Price, End Price, and Rate of Change (aka the starting price of the S&P 500 when the US market opened on a specific day, the ending price of the S&P 500 when the US market closed on the same specific day, and the percentage change that this stock exhibited between the start and close times of that specific day (100 * (end price - start price) / start price)).\n",
    "\n",
    "- For both the merged_df and sample_companies dataset, each row corresponds to an unique, mid- to large-cap company that is publicly-traded and utilizes USD. merged_df, our main dataset, has 620 companies, while sample_companies has 30 companies for now. merged_df and sample_companies have the same columns, they are:\n",
    "+ ticker - a unique combo of letters and numbers that represent a particular stock\n",
    "+ name - the official name of the company\n",
    "+ currency - the currency the company is traded in (this was filtered to only be USD)\n",
    "+ exchange - what market the company is exchanged on\n",
    "+ industry - the type of output the company produces\n",
    "+ logo - a link to the company logo, potentially for joining with other datasets (MIGHT BE DROPPED LATER)\n",
    "+ weburl - a link to the company website, potentially for joining with other datasets or scraping for text sentiment analysis (MIGHT BE DROPPED LATER)\n",
    "+ environment_grade - a letter score given to the company that measures how well it complies to environmental standards, ranging from AAA being the best to CCC being the worst\n",
    "+ environment_level - a categorical classification of a company's overall environmental performance (low, medium, high)\n",
    "+ social_grade - a letter score given to the company that measures how well it complies to social standards, ranging from AAA being the best to CCC being the worst\n",
    "+ social_level - a categorical classification of a company's overall social performance (low, medium, high)\n",
    "+ governance_grade - a letter score given to the company that measures how well it complies to governance standards, ranging from AAA being the best to CCC being the worst\n",
    "+ governance_level - a categorical classification of a company's overall governance performance (low, medium, high)\n",
    "+ environment_score - a numerical measure of how well a company performs on environment-related factors, ranging from 0-1000\n",
    "+ social_score - a numerical measure of how well a company performs on social-related factors, ranging from 0-1000\n",
    "+ governance_score - a numerical measure of how well a company performs on governance-related factors, ranging from 0-1000\n",
    "+ total_score - a numerical measure of how well a company performs on environment, social, and governance-related factors, ranging from 0-1500]\n",
    "+ cik - central index key, a unique identifier assigned by the SEC to any company that files documents with the SEC (MIGHT BE DROPPED LATER)\n",
    "the following columns are ones that we created, using the yfinance data:\n",
    "+ percent_change - the percent change in the company stock price from close time on 2/1/21 to close time on 12/31/22 (100 * (end price - start price) / start price))\n",
    "+ start_close - the closing price of the company stock on 2/1/21\n",
    "+ end_close - the closing price of the company stock on 12/31/22\n",
    "+ volatility - standard deviation of daily returns of the company stock, aka the percentage change in the stock price from day to day (indicator of how much stock price fluctuates in a given period, higher volatility is riskier, lower volatility has more stability). specifically, daily return is calculated by closing price on day x+1 - closing price on day x divided by closing price on day x, so all daily returns in the time period 2/1/21-12/31/22 are calculated for the specific company stock, and then the standard deviation is taken to get the volatility\n",
    "+ 50_day_SMA - 50 day simple moving average, or the sum of closing price of a company stock for the last 50 days before 12/31/22, divided by 50 (if current stock price is above the 50-day SMA, the company is in uptrend, and vice versa)\n",
    "+ cumulative_return - cumulative return of the company stock over the entire period ((close price on 12/31/22 / close price on 2/1/21) - 1), positive values represent returns, and negative values represent losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### __Data Limitations__\n",
    "\n",
    "1. ESG is typically evaluated annually, which might mean the scores in our dataset don't reflect the most accurate performance of the company, which directly impacts the analyses and conclusions we might draw from our EDA. in other words, when considering the short-term impact of the company's ESG and other policies, it's likely that policy change affect stocks immediately, but these changes might not also be reflected in the company's ESG rating. Bascially, since ESG scores lag behind the stock fluctuations due to immediate events (mergers, acqusitions, freak events like the CrowdStrike failure), any significant events that occur during 2/1/21-12/31/22 may result in stock price changes that do not perfectly correlate to ESG metrics. This could skew our correlation or regression model analysis, so we want to be careful to not falsely attributing any stock changes to ESG scores (in case of possible confounding variables). As a caveat, we should also be careful not to infer any causal relations when correlation exists.\n",
    "\n",
    "2. ESG is a constant value that is gathered from different days for each company in 2022, though stock prices for these companies change over time every day. We cannot perform any time-series analyses with ESG due to this fact, which limits what we can do for our final phases and EDA.\n",
    "   \n",
    "3. Due to the nature of the Kaggle csv and yfinance data, our data is restricted to the variable types of stock data, industry type, company name, and ESG score - which actually does help us narrow down the scope of our research question, but limits the breadth of the analyses we can perform as well.\n",
    "   \n",
    "4. Some specific data from the yfinance library is missing - we had to drop all companies that didn't have the specified data we wanted in our specified time range. We also had to drop all companies from the original ESG csv that had missing or blank data. Overall, this means that our analyses will not be perfectly representative of all companies that use USD and have an ESG rating (can't perfectly generalize to the population). Additionally, we filtered our original ESG dataframe to be just companies traded in USD, so we can't do any inter-country comparison (although this also helps us narrow the scope of our project). Since we are also only using companies that are existent and large/mid-cap within 2/1/21 - 12/31/22, any company that stopped their operations in this time frame will be excluded. In other words, our findings might disproportionally overestimate the relationship between ESG scores and stock performance (companies that went bankrupt or have poor ESG / financial outcomes are not represented, which might skew interpretation of ESG positively).\n",
    "   \n",
    "5. For our sample dataset (companies_sample), the current 30 companies were chosen with a stratified sample. As a reminder, essentially, we looked at all of the different industries, and ranked them by total ESG score. then, we divided up the industries into 3 groups: high ESG score, medium ESG score, and low ESG score. Then, from each group, we chose 2 industries from a random sample. After this, we then randomly selected 5 companies from each industry, giving us a dataset of 30 companies. This sample is not fully representative of all USD-using companies with ESG ratings (this actually ties into one of our questions for reviewers). After getting feedback, we may also consider complete random sampling (no stratification) to expand the representativeness of our sample dataset, but since we are exploring potential connections between ESG ratings and company stock performance, we may need to sample not only by industry but also by ESG rating levels to ensure a more balanced and comprehensive analysis of the different ESG performance tiers (for our sample dataset). Furthermore, though our sample size is sufficient to assume normality in distribution, it is still rather small compared to merged_df's number of companies included, so our findings might reflect trends only in specific industries or companies rather than the average and general market trends.\n",
    "   \n",
    "6. We currently plan on comparing the rate of change of the sample stocks (in companies_sample) to the S&P 500's rate of change. We also plan on taking a look at volatility, cumulative returns, and the 50 day simple moving average, but other measures of stock performance might provide more valuable insights (but we will proceed with these 4 for now). Additionally, due to the last processing date of the ESG scores for the companies, we also restricted our stock data to be from 2/1/21 - 12/31/22, which poses a limitation on the amount of analyses we can garner as we cannot extrapolate our conclusions to beyond this time frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Preregistration Statements\n",
    "\n",
    "#### Hypothesis 1: \n",
    "Industries with higher total ESG scores will have generally better stock performances.\n",
    "\n",
    "#### Analysis 1: \n",
    "According to La Torre, Mango, Cafaro, and Leo (2020), “panel data analysis demonstrates that the ‘ESG Overall index’ has, in general, a statistically significant and positive impact on returns (on a company level).” We would like to use our dataset to explore if there is statistical evidence showing ESG’s correlation with stock return on an industry level. To achieve this, we will run both Linear Regression and Logistic Regression where we input the average total ESG scores [‘Average Total ESG Score’, from avg_esg_by_industry ] for each industry, and output the expected stock performance (both for the stock rate of change over the time period [percent_change] and the stock 50-Day Simple Moving Average [50_day_SMA], which are two different dimensions illustrating financial / stock performance). In this case, we want to test if βESG > 0 at a significance level of 0.05 - this will be 4 different tests.\n",
    "\n",
    "\n",
    "### Preregistration Statement 2\n",
    "#### Hypothesis 2\n",
    "Industries with 'High' ESG score levels exhibit a higher correlation coefficient between their stock's rate of change over the specified time period and the ESG total score than industries with 'Low' or 'Medium' ESG score levels. We anticipate this pattern to hold across both our full dataset (`merged_df`) and our sample dataset (`sample_companies`).\n",
    "\n",
    "#### Analysis 2\n",
    "Building on findings from Friede, G., Busch, T., & Bassen, A. (2015) that suggest a positive relationship between ESG and company performance in over 2100 studies, we aim to examine whether ESG score levels correlate with stock return rate more strongly in industries with higher ESG scores. We will conduct two hypothesis tests, setting our alternative hypothesis as follows: the correlation coefficient (between stock rate of change, `percentage_change`, and total ESG score, `total_score`) is higher for industries with “High” ESG scores than for those with “Medium” or “Low” scores.\n",
    "\n",
    "To test this, we will use a Fisher Z-transformation model to statistically compare the correlation coefficients. We’ll determine whether the difference between the 'High' ESG score group and the combined 'Medium' and 'Low' groups is significant at a 95% confidence level. One test will be applied to the full dataset (`merged_df`), and the other to the sample dataset (`sample_companies`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Analysis:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Results for Percent Change:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:      percentage_change   R-squared:                       0.242\n",
      "Model:                            OLS   Adj. R-squared:                  0.223\n",
      "Method:                 Least Squares   F-statistic:                     13.07\n",
      "Date:                Thu, 14 Nov 2024   Prob (F-statistic):           0.000812\n",
      "Time:                        16:33:20   Log-Likelihood:                -199.69\n",
      "No. Observations:                  43   AIC:                             403.4\n",
      "Df Residuals:                      41   BIC:                             406.9\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==================================================================================================\n",
      "                                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "const                              2.7675      3.928      0.704      0.485      -5.166      10.701\n",
      "average_total_esg_score_scaled    14.2038      3.928      3.616      0.001       6.270      22.137\n",
      "==============================================================================\n",
      "Omnibus:                       30.666   Durbin-Watson:                   2.296\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               72.608\n",
      "Skew:                           1.867   Prob(JB):                     1.71e-16\n",
      "Kurtosis:                       8.156   Cond. No.                         1.00\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "Logistic Regression Coefficients for 50-Day SMA Performance:\n",
      "Intercept: -0.04728915540411187\n",
      "Coefficient for ESG Score: 0.35350973814072384\n",
      "\n",
      "Classification Report for Logistic Regression Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.68      0.67        22\n",
      "           1       0.65      0.62      0.63        21\n",
      "\n",
      "    accuracy                           0.65        43\n",
      "   macro avg       0.65      0.65      0.65        43\n",
      "weighted avg       0.65      0.65      0.65        43\n",
      "\n",
      "Linear Regression Test for βESG > 0\n",
      "ESG Coefficient: 14.203794810721217\n",
      "T-statistic: 3.615761835371704\n",
      "One-tailed p-value: 0.000406024421704676\n",
      "Reject the null hypothesis: ESG has a positive effect on percent change.\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.674776\n",
      "         Iterations 4\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:        SMA_performance   No. Observations:                   43\n",
      "Model:                          Logit   Df Residuals:                       41\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Thu, 14 Nov 2024   Pseudo R-squ.:                 0.02612\n",
      "Time:                        16:33:20   Log-Likelihood:                -29.015\n",
      "converged:                       True   LL-Null:                       -29.794\n",
      "Covariance Type:            nonrobust   LLR p-value:                    0.2122\n",
      "==================================================================================================\n",
      "                                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "const                             -0.0479      0.311     -0.154      0.878      -0.657       0.561\n",
      "average_total_esg_score_scaled     0.3893      0.319      1.220      0.222      -0.236       1.015\n",
      "==================================================================================================\n",
      "One-tailed p-value for ESG coefficient in Logistic Regression: 0.11119373562294926\n",
      "Fail to reject the null hypothesis: No significant positive effect of ESG on 50-Day SMA performance classification.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Aggregate the data at the industry level\n",
    "industry_data = merged_df.groupby('industry').agg({\n",
    "    'total_score': 'mean',\n",
    "    'percentage_change': 'mean',\n",
    "    '50_day_SMA': 'mean'\n",
    "}).rename(columns={'total_score': 'average_total_esg_score'})\n",
    "\n",
    "# Step 2: Standardize the predictor (average_total_esg_score)\n",
    "scaler = StandardScaler()\n",
    "industry_data['average_total_esg_score_scaled'] = scaler.fit_transform(industry_data[['average_total_esg_score']])\n",
    "\n",
    "# Step 3: Linear Regression for percent_change\n",
    "X_linear = industry_data[['average_total_esg_score_scaled']]\n",
    "y_linear = industry_data['percentage_change']\n",
    "X_linear = sm.add_constant(X_linear)  # Add intercept\n",
    "\n",
    "linear_model = sm.OLS(y_linear, X_linear).fit()\n",
    "print(\"Linear Regression Results for Percent Change:\")\n",
    "print(linear_model.summary())\n",
    "\n",
    "# Step 4: Logistic Regression for 50_day_SMA as binary performance (above/below median)\n",
    "median_SMA = industry_data['50_day_SMA'].median()\n",
    "industry_data['SMA_performance'] = (industry_data['50_day_SMA'] > median_SMA).astype(int)\n",
    "\n",
    "X_logistic = industry_data[['average_total_esg_score_scaled']]\n",
    "y_logistic = industry_data['SMA_performance']\n",
    "\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_logistic, y_logistic)\n",
    "\n",
    "# Print Logistic Regression results\n",
    "print(\"Logistic Regression Coefficients for 50-Day SMA Performance:\")\n",
    "print(f\"Intercept: {logistic_model.intercept_[0]}\")\n",
    "print(f\"Coefficient for ESG Score: {logistic_model.coef_[0][0]}\")\n",
    "\n",
    "# Classification report for logistic model\n",
    "y_pred = logistic_model.predict(X_logistic)\n",
    "print(\"\\nClassification Report for Logistic Regression Model:\")\n",
    "print(classification_report(y_logistic, y_pred))\n",
    "\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Linear Regression - Hypothesis test for βESG > 0\n",
    "esg_coefficient = linear_model.params['average_total_esg_score_scaled']\n",
    "esg_standard_error = linear_model.bse['average_total_esg_score_scaled']\n",
    "\n",
    "# Calculate the t-statistic for a one-tailed test\n",
    "t_statistic = esg_coefficient / esg_standard_error\n",
    "\n",
    "# Get the p-value for the one-tailed test (βESG > 0)\n",
    "p_value_one_tailed = 1 - stats.t.cdf(t_statistic, df=linear_model.df_resid)\n",
    "\n",
    "print(\"Linear Regression Test for βESG > 0\")\n",
    "print(f\"ESG Coefficient: {esg_coefficient}\")\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"One-tailed p-value: {p_value_one_tailed}\")\n",
    "\n",
    "if p_value_one_tailed < 0.05:\n",
    "    print(\"Reject the null hypothesis: ESG has a positive effect on percent change.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: No significant positive effect of ESG on percent change.\")\n",
    "\n",
    "\n",
    "# Logistic Regression using statsmodels for p-values\n",
    "X_logistic_sm = sm.add_constant(X_logistic)  # Add intercept for statsmodels\n",
    "logit_model = sm.Logit(y_logistic, X_logistic_sm).fit()\n",
    "print(logit_model.summary())\n",
    "\n",
    "# Extract one-tailed p-value for ESG coefficient\n",
    "esg_logit_p_value_one_tailed = logit_model.pvalues['average_total_esg_score_scaled'] / 2\n",
    "print(f\"One-tailed p-value for ESG coefficient in Logistic Regression: {esg_logit_p_value_one_tailed}\")\n",
    "\n",
    "if esg_logit_p_value_one_tailed < 0.05:\n",
    "    print(\"Reject the null hypothesis: ESG has a positive effect on 50-Day SMA performance classification.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: No significant positive effect of ESG on 50-Day SMA performance classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preregistration Statement 2\n",
    "\n",
    "#the merged_df\n",
    "# Step 1: Define the two groups based on ESG levels\n",
    "pop_high = merged_df[merged_df['total_grade'] == 'BBB']\n",
    "pop_not_high = merged_df[(merged_df['total_grade'] == 'BB') | (merged_df['total_grade'] == 'B')]\n",
    "\n",
    "# Correlation between `percentage_change` and `total_score`\n",
    "pop_r_high = pop_high['percentage_change'].corr(pop_high['total_score'])\n",
    "pop_r_not_high = pop_not_high['percentage_change'].corr(pop_not_high['total_score'])\n",
    "\n",
    "# Sample sizes for each group\n",
    "pop_n_high = pop_high.shape[0]\n",
    "pop_n_not_high = pop_not_high.shape[0]\n",
    "\n",
    "#the sample_companies\n",
    "# Step 1: Define the two groups based on ESG levels\n",
    "sample_high = sample_companies[sample_companies['total_grade'] == 'BBB']\n",
    "sample_not_high = sample_companies[(sample_companies['total_grade'] == 'BB') | (sample_companies['total_grade'] == 'B')]\n",
    "\n",
    "# Correlation between `percentage_change` and `total_score`\n",
    "sample_r_high = sample_high['percentage_change'].corr(sample_high['total_score'])\n",
    "sample_r_not_high = sample_not_high['percentage_change'].corr(sample_not_high['total_score'])\n",
    "\n",
    "# Sample sizes for each group\n",
    "sample_n_high = sample_high.shape[0]\n",
    "sample_n_not_high = sample_not_high.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_test_correlation(n1, r1, n2, r2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform a Z-test for the difference between two correlation coefficients.\n",
    "    \n",
    "    Parameters:\n",
    "    n1 (int): Sample size of the first group\n",
    "    r1 (float): Correlation coefficient of the first group\n",
    "    n2 (int): Sample size of the second group\n",
    "    r2 (float): Correlation coefficient of the second group\n",
    "    alpha (float): Significance level for hypothesis testing (default is 0.05)\n",
    "    \n",
    "    Returns:\n",
    "    float: Z-score for the difference in correlations\n",
    "    float: p-value for the test\n",
    "    str: Interpretation of the result\n",
    "    \"\"\"\n",
    "    # Step 1: Convert correlation coefficients to Fisher's Z scores\n",
    "    z1 = np.arctanh(r1)\n",
    "    z2 = np.arctanh(r2)\n",
    "    \n",
    "    # Step 2: Calculate the standard error for the difference in Z scores\n",
    "    se_diff = np.sqrt(1 / (n1 - 3) + 1 / (n2 - 3))\n",
    "    \n",
    "    # Step 3: Calculate the Z score for the difference between the two correlation coefficients\n",
    "    z_score = (z1 - z2) / se_diff\n",
    "    print(f\"Z-score is : {z_score}\")\n",
    "    \n",
    "    # Step 4: Calculate the p-value (two-tailed)\n",
    "    p_value = 2 * (1 - norm.cdf(abs(z_score)))\n",
    "    print(f'P value is: {p_value}')\n",
    "    \n",
    "    # Step 5: Interpretation\n",
    "    if p_value < alpha:\n",
    "        interpretation = \"The difference in correlation coefficients is statistically significant.\"\n",
    "    else:\n",
    "        interpretation = \"The difference in correlation coefficients is not statistically significant.\"\n",
    "    \n",
    "    return interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z-score is : 1.2061190397733668\n",
      "P value is: 0.22777159449607853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The difference in correlation coefficients is not statistically significant.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#perform z-test:\n",
    "z_test_correlation(pop_n_high, pop_r_high, pop_n_not_high, pop_r_not_high)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z-score is : -0.5858445861831136\n",
      "P value is: 0.5579799584750469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The difference in correlation coefficients is not statistically significant.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_test_correlation(sample_n_high, sample_r_high, sample_n_not_high, sample_r_not_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypo 3 (undecided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the merged_df\n",
    "#mean\n",
    "pop_return_high = pop_high['percentage_change'].mean()\n",
    "pop_return_not_high = pop_not_high['percentage_change'].mean()\n",
    "#std\n",
    "\n",
    "pop_return_high_std = pop_high['percentage_change'].std()\n",
    "pop_return_not_high_std = pop_not_high['percentage_change'].std()\n",
    "\n",
    "# Sample sizes for each group\n",
    "pop_n_high = pop_high.shape[0]\n",
    "pop_n_not_high = pop_not_high.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_test_mean_greater(mean1, std1, n1, mean2, std2, n2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform a one-tailed Z-test to check if the mean of the first group is greater than the mean of the second group.\n",
    "    \n",
    "    Parameters:\n",
    "    mean1 (float): Mean of the first group\n",
    "    std1 (float): Standard deviation of the first group\n",
    "    n1 (int): Sample size of the first group\n",
    "    mean2 (float): Mean of the second group\n",
    "    std2 (float): Standard deviation of the second group\n",
    "    n2 (int): Sample size of the second group\n",
    "    alpha (float): Significance level for hypothesis testing (default is 0.05)\n",
    "    \n",
    "    Returns:\n",
    "    float: Z-score for the difference in means\n",
    "    float: p-value for the test\n",
    "    str: Interpretation of the result\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate the standard error of the difference between the means\n",
    "    se_diff = np.sqrt((std1**2 / n1) + (std2**2 / n2))\n",
    "    \n",
    "    # Step 2: Calculate the Z score for the difference between the two means\n",
    "    z_score = (mean1 - mean2) / se_diff\n",
    "    \n",
    "    # Step 3: Calculate the p-value (one-tailed test)\n",
    "    p_value = 1 - norm.cdf(z_score)  # Only interested in the probability of mean1 > mean2\n",
    "    \n",
    "    # Step 4: Interpretation\n",
    "    if p_value < alpha:\n",
    "        interpretation = \"The mean of the first group is significantly greater than the mean of the second group.\"\n",
    "    else:\n",
    "        interpretation = \"The mean of the first group is not significantly greater than the mean of the second group.\"\n",
    "    \n",
    "    return z_score, p_value, interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(3.8393115812888565),\n",
       " np.float64(6.168988626886751e-05),\n",
       " 'The mean of the first group is significantly greater than the mean of the second group.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hypothesis 3 test \n",
    "z_test_mean_greater(pop_return_high, pop_return_high_std, pop_n_high, pop_return_not_high, pop_return_not_high_std, pop_n_not_high)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluation of significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis 2:\n",
    "The Z-test results shows that, the p-values for both the full population (merged_df) and sample datasets (sample_companies) are not statistically significant at a 5% significance level (p-values are 0.2278 for population test and 0.5580 for sample test). This suggests that we fail to reject the null hypothesis and conclude that there is no significant difference in the correlation coefficients between the \"High\" ESG group and the combined \"Medium\" and \"Low\" ESG groups. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Limitations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Acknowledgements and Bibliography "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info2950",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
