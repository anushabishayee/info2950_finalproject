{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 2950: Data Cleaning\n",
    "#### Group Members: Anusha Bishayee, Katheryn Ding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### __Data Collection and Cleaning:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import yfinance as yf\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our original dataset with ESG information for different large/mid-cap companies came in a csv format, which we downloaded from Kaggle. this had about 722 rows, each corresponding to a unique publicly traded company. further description of the columns here can be found in the 'Dataset Description' portion of this notebook.\n",
    "\n",
    "we first dropped all rows that had null values, which eliminated 27 companies. we then filtered this dataset for just USD currency, excluding companies that are traded in CNY or any other currency. this allows us to have greater familiarity with the industries and companies we analyze - this process eliminated 15 more of our rows, and left us with 680 companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data shape: (722, 21)\n",
      "non-null data shape: (695, 21)\n",
      "refined data shape: (680, 21)\n"
     ]
    }
   ],
   "source": [
    "esg = pd.read_csv(\"esg_data.csv\")\n",
    "print(f\"original data shape: {esg.shape}\")\n",
    "\n",
    "esg = esg.dropna()\n",
    "print(f\"non-null data shape: {esg.shape}\")\n",
    "\n",
    "esg = esg[esg[\"currency\"] == \"USD\"]\n",
    "print(f\"refined data shape: {esg.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then, we converted the 'last_processing_date' column in our dataset to DateTime format - a lot of rows had a differing date formats as well, so we had to convert them all to m/d/y. after that, we sorted the dataset by ascending and descending 'last_processing_date' to see the range of processing dates in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest dates:\n",
      "720    11-15-2022\n",
      "716    11-15-2022\n",
      "Name: last_processing_date, dtype: object\n",
      "\n",
      "earliest dates:\n",
      "658    02-08-2022\n",
      "36     04-16-2022\n",
      "Name: last_processing_date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "esg[\"last_processing_date\"] = pd.to_datetime(esg[\"last_processing_date\"], format = \"mixed\")\n",
    "esg[\"last_processing_date\"] = esg[\"last_processing_date\"].dt.strftime('%m-%d-%Y')\n",
    "\n",
    "esg = esg.sort_values(by = \"last_processing_date\", ascending = False)\n",
    "print(f\"latest dates:\\n{esg[\"last_processing_date\"].head(2)}\")\n",
    "\n",
    "esg = esg.sort_values(by = \"last_processing_date\", ascending = True)\n",
    "print(f\"\\nearliest dates:\\n{esg[\"last_processing_date\"].head(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after this, we realized we have two 'Energy' values for the 'industry' column - one is 'Energy ' and one is 'Energy'. we renamed all the 'Energy ' values, and also re-formatted some other industry column values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Leisure Products' 'Semiconductors' 'Health Care' 'Chemicals'\n",
      " 'Telecommunication' 'Consumer Products' 'Airlines' 'Insurance'\n",
      " 'Communications' 'Building' 'Technology' 'Logistics & Transportation'\n",
      " 'Biotechnology' 'Banking' 'Pharmaceuticals' 'Financial Services'\n",
      " 'Life Sciences Tools & Services' 'Electrical Equipment' 'Real Estate'\n",
      " 'Machinery' 'Retail' 'Food Products' 'Industrial Conglomerates'\n",
      " 'Hotels, Restaurants, & Leisure' 'Utilities' 'Beverages' 'Tobacco'\n",
      " 'Media' 'Auto Components' 'Energy' 'Commercial Services & Supplies'\n",
      " 'Packaging' 'Road & Rail' 'Metals & Mining'\n",
      " 'Textiles, Apparel, & Luxury Goods' 'Trading Companies & Distributors'\n",
      " 'Aerospace & Defense' 'Automobiles' 'Distributors'\n",
      " 'Professional Services' 'Construction' 'Marine'\n",
      " 'Diversified Consumer Services']\n"
     ]
    }
   ],
   "source": [
    "esg['industry'] = esg['industry'].replace('Energy ', 'Energy')\n",
    "esg['industry'] = esg['industry'].replace('Hotels, Restaurants & Leisure', 'Hotels, Restaurants, & Leisure')\n",
    "esg['industry'] = esg['industry'].replace('Hotels Restaurants and Leisure', 'Hotels, Restaurants, & Leisure')\n",
    "esg['industry'] = esg['industry'].replace('Consumer products', 'Consumer Products')\n",
    "esg['industry'] = esg['industry'].replace('Logistics and Transportation', 'Logistics & Transportation')\n",
    "esg['industry'] = esg['industry'].replace('Life Sciences Tools and Services', 'Life Sciences Tools & Services')\n",
    "esg['industry'] = esg['industry'].replace('Commercial Services and Supplies', 'Commercial Services & Supplies')\n",
    "esg['industry'] = esg['industry'].replace('Road and Rail', 'Road & Rail')\n",
    "esg['industry'] = esg['industry'].replace('Metals and Mining', 'Metals & Mining')\n",
    "esg['industry'] = esg['industry'].replace('Aerospace and Defense', 'Aerospace & Defense')\n",
    "esg['industry'] = esg['industry'].replace('Textiles Apparel and Luxury Goods', 'Textiles, Apparel, & Luxury Goods')\n",
    "esg['industry'] = esg['industry'].replace('Trading Companies and Distributors', 'Trading Companies & Distributors')\n",
    "\n",
    "print(esg['industry'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, we wanted to add our finance data from the yfinance library onto to our esg dataset. we used the ticker column to match up companies from the yfinance library and our esg dataset, and we set our dates of the finance data to range from 2/1/21 to 12/31/22, as all of the 'last processing date' values for the esg data range from 2/8/22 to 11/15/22. in specific, we calculated a stock percentage change over this period for each company, a volatility index, a 50-day moving average, and a cumulative return metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevents some annoying yfinance outputs from printing, gpt was used to assist here\n",
    "@contextlib.contextmanager\n",
    "def suppress_output():\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        old_stderr = sys.stderr\n",
    "        sys.stdout = devnull\n",
    "        sys.stderr = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "            sys.stderr = old_stderr\n",
    "\n",
    "tickers = esg[\"ticker\"].tolist()\n",
    "stock_data = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        # suppresses all of the outputs when grabbing data from yfinance\n",
    "        with suppress_output():  \n",
    "            stock = yf.download(ticker, start = \"2021-02-01\", end = \"2022-12-31\", progress = False)\n",
    "        \n",
    "        if not stock.empty:\n",
    "            # get closing price for 01/01/2021 and 12/31/2022\n",
    "            close_2021_02_01 = stock.loc[\"2021-02-01\"][\"Close\"] if \"2021-02-01\" in stock.index else None\n",
    "            close_2022_12_31 = stock.loc[\"2022-12-30\"][\"Close\"] if \"2022-12-30\" in stock.index else None\n",
    "\n",
    "            # calculating percentage change\n",
    "            percentage_change = ((stock[\"Close\"].iloc[-1] - stock[\"Close\"].iloc[0]) / stock[\"Close\"].iloc[0]) * 100\n",
    "            \n",
    "            # calculating volatility (sd of daily returns)\n",
    "            daily_returns = stock[\"Close\"].pct_change()\n",
    "            volatility = daily_returns.std()\n",
    "            \n",
    "            # calculating 50-day moving average\n",
    "            stock[\"50_day_SMA\"] = stock[\"Close\"].rolling(window=50).mean()\n",
    "            sma_50_day = stock[\"50_day_SMA\"].iloc[-1]\n",
    "            \n",
    "            # calculating cumulative return\n",
    "            cumulative_return = (stock[\"Close\"].iloc[-1] / stock[\"Close\"].iloc[0]) - 1\n",
    "            \n",
    "            stock_data.append({\n",
    "                'ticker': ticker, \n",
    "                'start_close': close_2021_02_01,\n",
    "                'end_close': close_2022_12_31,\n",
    "                'percentage_change': percentage_change,\n",
    "                'volatility': volatility,\n",
    "                '50_day_SMA': sma_50_day,\n",
    "                'cumulative_return': cumulative_return\n",
    "            })\n",
    "            \n",
    "    # also helps to suppress annoying outputs\n",
    "    except (yf.YFTzMissingError, yf.YFPricesMissingError):\n",
    "        pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, we needed to convert the stock data we extracted from yfinance to a dataframe, so that we can merge it with our original esg dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current data shape: (680, 27)\n",
      "\n",
      "  ticker                           name currency  \\\n",
      "0   poww                       Ammo Inc      USD   \n",
      "1   acls       Axcelis Technologies Inc      USD   \n",
      "2   achc  Acadia Healthcare Company Inc      USD   \n",
      "3     cf     CF Industries Holdings Inc      USD   \n",
      "4      t                       AT&T Inc      USD   \n",
      "\n",
      "                        exchange           industry  \\\n",
      "0     NASDAQ NMS - GLOBAL MARKET   Leisure Products   \n",
      "1     NASDAQ NMS - GLOBAL MARKET     Semiconductors   \n",
      "2     NASDAQ NMS - GLOBAL MARKET        Health Care   \n",
      "3  NEW YORK STOCK EXCHANGE, INC.          Chemicals   \n",
      "4  NEW YORK STOCK EXCHANGE, INC.  Telecommunication   \n",
      "\n",
      "                                                logo  \\\n",
      "0  https://static.finnhub.io/logo/8decc6ca0564a89...   \n",
      "1  https://static.finnhub.io/logo/88b5f730-80df-1...   \n",
      "2  https://static.finnhub.io/logo/4b6b2e5a4cfce5b...   \n",
      "3  https://static.finnhub.io/logo/9b57a636-80eb-1...   \n",
      "4  https://static.finnhub.io/logo/7d20269e-80ec-1...   \n",
      "\n",
      "                              weburl environment_grade environment_level  \\\n",
      "0               https://ammoinc.com/                 B            Medium   \n",
      "1           https://www.axcelis.com/                 A              High   \n",
      "2  https://www.acadiahealthcare.com/                BB            Medium   \n",
      "3      https://www.cfindustries.com/                 A              High   \n",
      "4               https://www.att.com/                 B            Medium   \n",
      "\n",
      "  social_grade  ... last_processing_date total_grade total_level      cik  \\\n",
      "0            B  ...           02-08-2022           B      Medium  1015383   \n",
      "1           BB  ...           04-16-2022         BBB        High  1113232   \n",
      "2            B  ...           04-16-2022          BB      Medium  1520697   \n",
      "3           BB  ...           04-16-2022         BBB        High  1324404   \n",
      "4            B  ...           04-16-2022           B      Medium   732717   \n",
      "\n",
      "   start_close  end_close  percentage_change volatility 50_day_SMA  \\\n",
      "0     5.400000   1.730000         -67.962963   0.040577     2.3782   \n",
      "1    36.150002  79.360001         119.529730   0.038259    73.6454   \n",
      "2    52.990002  82.320000          55.350061   0.021303    82.6706   \n",
      "3    42.970001  85.199997          98.277856   0.027433   101.2440   \n",
      "4    21.638973  18.410000         -14.922027   0.015132    18.5720   \n",
      "\n",
      "  cumulative_return  \n",
      "0         -0.679630  \n",
      "1          1.195297  \n",
      "2          0.553501  \n",
      "3          0.982779  \n",
      "4         -0.149220  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "stock_df = pd.DataFrame(stock_data)\n",
    "merged_df = esg.merge(stock_df, on = 'ticker', how = 'left')\n",
    "print(f\"current data shape: {merged_df.shape}\")\n",
    "print(f\"\\n{merged_df.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after that, we needed to drop any rows where the finance data join has left null values. this eliminates 80 more of our rows, leaving us with 600 companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-null finance data shape: (595, 27)\n",
      "\n",
      "<bound method NDFrame.head of     ticker                             name currency  \\\n",
      "71     mmm                            3M Co      USD   \n",
      "142    aos                   A O Smith Corp      USD   \n",
      "661   abvc               ABVC Biopharma Inc      USD   \n",
      "17    acad       ACADIA Pharmaceuticals Inc      USD   \n",
      "41    aciw                ACI Worldwide Inc      USD   \n",
      "..     ...                              ...      ...   \n",
      "500    zts                       Zoetis Inc      USD   \n",
      "571    zuo                        Zuora Inc      USD   \n",
      "643    zws  Zurn Elkay Water Solutions Corp      USD   \n",
      "647   zyme                    Zymeworks Inc      USD   \n",
      "189   ebay                         eBay Inc      USD   \n",
      "\n",
      "                          exchange                  industry  \\\n",
      "71   NEW YORK STOCK EXCHANGE, INC.  Industrial Conglomerates   \n",
      "142  NEW YORK STOCK EXCHANGE, INC.                  Building   \n",
      "661     NASDAQ NMS - GLOBAL MARKET             Biotechnology   \n",
      "17      NASDAQ NMS - GLOBAL MARKET             Biotechnology   \n",
      "41      NASDAQ NMS - GLOBAL MARKET                Technology   \n",
      "..                             ...                       ...   \n",
      "500  NEW YORK STOCK EXCHANGE, INC.           Pharmaceuticals   \n",
      "571  NEW YORK STOCK EXCHANGE, INC.                Technology   \n",
      "643  NEW YORK STOCK EXCHANGE, INC.                  Building   \n",
      "647  NEW YORK STOCK EXCHANGE, INC.             Biotechnology   \n",
      "189     NASDAQ NMS - GLOBAL MARKET                    Retail   \n",
      "\n",
      "                                                  logo  \\\n",
      "71   https://static.finnhub.io/logo/2a1802fa-80ec-1...   \n",
      "142  https://static.finnhub.io/logo/73381be8-80eb-1...   \n",
      "661  https://static.finnhub.io/logo/2b1c9fbcfafa70d...   \n",
      "17   https://static.finnhub.io/logo/2d87da57d47f7a5...   \n",
      "41   https://static.finnhub.io/logo/875c5a76-80df-1...   \n",
      "..                                                 ...   \n",
      "500  https://static.finnhub.io/logo/aae505a6-80cd-1...   \n",
      "571  https://static.finnhub.io/logo/181c3c26-80db-1...   \n",
      "643  https://static.finnhub.io/logo/166822dc-80c9-1...   \n",
      "647  https://static.finnhub.io/logo/d5054e357d522c9...   \n",
      "189  https://static.finnhub.io/logo/919a6270-826a-1...   \n",
      "\n",
      "                              weburl environment_grade environment_level  \\\n",
      "71               https://www.3m.com/                 A              High   \n",
      "142         https://www.aosmith.com/                 A              High   \n",
      "661          https://abvcpharma.com/                 B            Medium   \n",
      "17               https://acadia.com/                 B            Medium   \n",
      "41     https://www.aciworldwide.com/                 A              High   \n",
      "..                               ...               ...               ...   \n",
      "500          https://www.zoetis.com/                 A              High   \n",
      "571           https://www.zuora.com/                 B            Medium   \n",
      "643  https://zurnwatersolutions.com/                 A              High   \n",
      "647       https://www.zymeworks.com/                 B            Medium   \n",
      "189         https://www.ebayinc.com/                 A              High   \n",
      "\n",
      "    social_grade  ... last_processing_date total_grade total_level      cik  \\\n",
      "71            BB  ...           04-16-2022         BBB        High    66740   \n",
      "142           BB  ...           04-16-2022         BBB        High    91142   \n",
      "661            B  ...           11-06-2022           B      Medium  1173313   \n",
      "17             B  ...           04-16-2022          BB      Medium  1070494   \n",
      "41            BB  ...           04-16-2022         BBB        High   935036   \n",
      "..           ...  ...                  ...         ...         ...      ...   \n",
      "500           BB  ...           04-20-2022         BBB        High  1555280   \n",
      "571            B  ...           09-24-2022           B      Medium  1423774   \n",
      "643           BB  ...           11-06-2022         BBB        High  1439288   \n",
      "647           BB  ...           11-06-2022          BB      Medium  1403752   \n",
      "189           BB  ...           04-17-2022         BBB        High  1065088   \n",
      "\n",
      "     start_close   end_close  percentage_change volatility  50_day_SMA  \\\n",
      "71    146.070236  100.267555         -31.356615   0.014472  104.066388   \n",
      "142    56.029999   57.240002           2.159563   0.018196   57.343600   \n",
      "661    50.000000    6.250000         -87.500000   0.089921    7.173200   \n",
      "17     46.580002   15.920000         -65.822242   0.045374   15.391400   \n",
      "41     39.590000   23.000000         -41.904522   0.021739   21.438400   \n",
      "..           ...         ...                ...        ...         ...   \n",
      "500   155.580002  146.550003          -5.804087   0.016947  147.633001   \n",
      "571    14.240000    6.360000         -55.337077   0.034177    7.057200   \n",
      "643    38.889999   21.150000         -45.615840   0.032776   23.315800   \n",
      "647    35.639999    7.860000         -77.946127   0.049518    7.356000   \n",
      "189    58.470001   41.470001         -29.074739   0.022532   42.343200   \n",
      "\n",
      "    cumulative_return  \n",
      "71          -0.313566  \n",
      "142          0.021596  \n",
      "661         -0.875000  \n",
      "17          -0.658222  \n",
      "41          -0.419045  \n",
      "..                ...  \n",
      "500         -0.058041  \n",
      "571         -0.553371  \n",
      "643         -0.456158  \n",
      "647         -0.779461  \n",
      "189         -0.290747  \n",
      "\n",
      "[595 rows x 27 columns]>\n"
     ]
    }
   ],
   "source": [
    "merged_df = merged_df.dropna()\n",
    "print(f\"non-null finance data shape: {merged_df.shape}\\n\")\n",
    "\n",
    "merged_df = merged_df.sort_values(by = \"name\", ascending = True)\n",
    "print(merged_df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ticker', 'name', 'currency', 'industry', 'environment_grade',\n",
       "       'environment_level', 'social_grade', 'social_level', 'governance_grade',\n",
       "       'governance_level', 'environment_score', 'social_score',\n",
       "       'governance_score', 'total_score', 'last_processing_date',\n",
       "       'total_grade', 'total_level', 'start_close', 'end_close',\n",
       "       'percentage_change', 'volatility', '50_day_SMA', 'cumulative_return'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = merged_df.drop(['exchange', 'logo', 'weburl','cik'], axis = 1)\n",
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is still a lot of data, but will be helpful for getting industry-level and other general overviews of the data. merged_df will be our main dataframe. now, we will convert this dataframe into .csv format to use in the next stages of our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('merged_df.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info2950",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
